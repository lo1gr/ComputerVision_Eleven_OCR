{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Group10_11deepCV.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bYS_KhqOkyW",
        "colab_type": "text"
      },
      "source": [
        "# Seven segment digit classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zX1bgJ9wP85n",
        "colab_type": "text"
      },
      "source": [
        "This notebook contains our development of several computer vision and deep learning programs to identify and classify numbers composed of seven segment digits in photos. \n",
        "\n",
        "We followed a 3 step methodology which is organised as follows:\n",
        "\n",
        "\n",
        "  1. Extract LCD screen\n",
        "  \n",
        "  2. Extract each digit\n",
        "  \n",
        "  3. Classifiy digit per digit\n",
        "\n",
        "\n",
        "For the first step (extracting LCD), we use angle detection. Within each photo, the angles the most to the left and right of the photo approximately correspond to each corner of the LCD screen.\n",
        "\n",
        "For the second step, we considered a simple approach to crop out each digit individually out of the extracted LCD screen. We simply divide the LCD in different proportion segments. Proportions were reverse engineered to get the best approximate crop. \n",
        "\n",
        "The third and last step corresponds to the transfer learning part. We used the Mobilenet model pre-trained on the Imagenet dataset as a basis for our classifier. The crops are then passed to the downloaded model, for which we defreeze some layers to adapt to our case, seven segment digits being uncommon in datasets for model training. \n",
        "\n",
        "You will find below the code we developed following this methodology in order. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HStpT9ttOsGU",
        "colab_type": "text"
      },
      "source": [
        "## Mount drive to load and save data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wq0nelhkO4Ja",
        "colab_type": "code",
        "outputId": "bd6bd214-6429-4fe3-85f7-ec27d4384cdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hPQR2CW7G0Z",
        "colab_type": "text"
      },
      "source": [
        "To be able to run this notebook, simply mount your drive with the chunk above and add the data folder to your drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7aVy9OiOvbU",
        "colab_type": "text"
      },
      "source": [
        "## Load packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85XdzOlWOkH9",
        "colab_type": "code",
        "outputId": "81090e43-d94c-4de7-c417-493947e23f62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        }
      },
      "source": [
        "import logging\n",
        "import shutil\n",
        "import imutils\n",
        "import glob\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt \n",
        "#matplotlib.use('agg')\n",
        "\n",
        "import cv2\n",
        "import skimage.filters as ft\n",
        "import scipy.spatial as sp\n",
        "from skimage.measure import label, regionprops\n",
        "from PIL import Image, ImageEnhance \n",
        "\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import keras.backend\n",
        "from keras.models import Sequential\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten\n",
        "from keras.layers.core import Dropout, Activation\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import Dense,GlobalAveragePooling2D\n",
        "from keras import regularizers\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import plot_model\n",
        "from keras.callbacks import TensorBoard,EarlyStopping\n",
        "from keras.backend.tensorflow_backend import set_session\n",
        "from keras.applications import MobileNet\n",
        "from keras.applications.mobilenet import preprocess_input\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpdLJnziWEH1",
        "colab_type": "text"
      },
      "source": [
        "## Extracting LCD screen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pISCLNxNXpGu",
        "colab_type": "text"
      },
      "source": [
        "### Extraction with angle identification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0FFFqM_WlQx",
        "colab_type": "text"
      },
      "source": [
        "First we define a class which we can apply automatically to all our input images. Note that all the preprocessing is integrated directly in this class and applied to each photo to increase chances of correctly isolating the LCD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2LceCYCpNy-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class frameExtractor_3:\n",
        "\n",
        "    def __init__(self, image=None, src_file_name=None, dst_file_name=None, return_image=False, output_shape =(400,100)):\n",
        "        \"\"\"\n",
        "        Use this class to extract the frame/LCD screen from the image. This is our step 1 for image preprocessing.\n",
        "        The final frame is extracted in grayscale.\n",
        "        Note that it works for the \"digital\" case and can be used for the \"analog\" case, but it is more efficient on the \"digital\" case.\n",
        "        :param image: RGB image (numpy array NxMx3) with a screen to extract. If image is None, the image will be extracted from src_filename\n",
        "        :param src_file_name: filename to load the source image where the screen needs to be extracted (e.g. HQ_digital/0a07d2cff5beb0580bca191427e8cd6e1a0eb678.jpg)\n",
        "        :param dst_file_name: filename to save the preprocessed image (e.g. HQ_digital_frame/0a07d2cff5beb0580bca191427e8cd6e1a0eb678.jpg\n",
        "        :param return_image: a boolean, if True extractAndSave returns an image (np. array) / if False it just saves the image.\n",
        "        :param output_shape: shape (in pxl) of the output image.\n",
        "        \"\"\"\n",
        "        if image is None :\n",
        "            self.image = cv2.imread(src_file_name)\n",
        "        else :\n",
        "            self.image = image\n",
        "        self.dst_file_name = dst_file_name\n",
        "        self.return_image = return_image\n",
        "        self.output_shape = output_shape\n",
        "        #self.raw_frame = None\n",
        "        #self.frame = None\n",
        "        #self.sliced_frame = None\n",
        "        \n",
        "    \n",
        "    def our_max(self, i):   \n",
        "      if len(i) == 0 :  \n",
        "        total_max = np.reshape(np.array([1,1,2,20]),(2,2))\n",
        "       \n",
        "      if len(i) != 0 :\n",
        "        col1 = i[:,1]\n",
        "        col1_max = max(col1)\n",
        "        total_max = i[np.where(col1 == col1_max)]\n",
        "       \n",
        "      if len(total_max) == 1: \n",
        "        added = np.reshape(np.array([2,20]),(1,2))\n",
        "        total_max =np.append(total_max,added,axis = 0)\n",
        "        \n",
        "        return total_max[1]\n",
        "      \n",
        "      return total_max[0]\n",
        "  \n",
        "    \n",
        "    def our_min(self, i):\n",
        "      if len(i) == 0 :  \n",
        "        total_min = np.reshape(np.array([1,1,2,20]),(2,2))\n",
        "       \n",
        "      if len(i) != 0 :\n",
        "        col1 = i[:,1]\n",
        "        col1_min = min(col1)\n",
        "        total_min = i[np.where(col1 == col1_min)]\n",
        "       \n",
        "      if len(total_min) == 1: \n",
        "        added = np.reshape(np.array([1,10]),(1,2))\n",
        "        total_min =np.append(total_min,added,axis = 0)\n",
        "        return total_min[1]\n",
        " \n",
        "      return total_min[1]\n",
        "\n",
        "    def create_point(self, image,y,x,size_point=2):\n",
        "    \n",
        "      height, width, c = image.shape\n",
        "      border_rect = (x-int(round(width/400)),y-int(round(width/400)))\n",
        "      border_rect2 = (x+int(round(width/400)),y+int(round(width/400)))\n",
        "      img = cv2.rectangle(image,border_rect,border_rect2,(0,250,0),3)\n",
        "      \n",
        "      return img\n",
        "  \n",
        "    def order_points(self, pts):\n",
        "\t# initialize a list of coordinates that will be ordered\n",
        "\t# such that the first entry in the list is the top-left,\n",
        "\t# the second entry is the top-right, the third is the\n",
        "\t# bottom-right, and the fourth is the bottom-left\n",
        "      rect = np.zeros((4, 2), dtype = \"float32\")\n",
        " \n",
        "\t# the top-left point will have the smallest sum, whereas\n",
        "\t# the bottom-right point will have the largest sum\n",
        "      s = pts.sum(axis = 1)\n",
        "      rect[0] = pts[np.argmin(s)]\n",
        "      rect[2] = pts[np.argmax(s)]\n",
        " \n",
        "\t# now, compute the difference between the points, the\n",
        "\t# top-right point will have the smallest difference,\n",
        "\t# whereas the bottom-left will have the largest difference\n",
        "      diff = np.diff(pts, axis = 1)\n",
        "      rect[1] = pts[np.argmin(diff)]\n",
        "      rect[3] = pts[np.argmax(diff)]\n",
        "\t# return the ordered coordinates\n",
        "      return rect\n",
        "    \n",
        "    def four_point_transform(self, image, pts):\n",
        "\t# obtain a consistent order of the points and unpack them\n",
        "\t# individually\n",
        "      rect = self.order_points(pts)\n",
        "      (tl, tr, br, bl) = rect\n",
        " \n",
        "\t# compute the width of the new image, which will be the\n",
        "\t# maximum distance between bottom-right and bottom-left\n",
        "\t# x-coordiates or the top-right and top-left x-coordinates\n",
        "      widthA = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))\n",
        "      widthB = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))\n",
        "      maxWidth = max(int(widthA), int(widthB))\n",
        " \n",
        "\t# compute the height of the new image, which will be the\n",
        "\t# maximum distance between the top-right and bottom-right\n",
        "\t# y-coordinates or the top-left and bottom-left y-coordinates\n",
        "      heightA = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))\n",
        "      heightB = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))\n",
        "      maxHeight = max(int(heightA), int(heightB))\n",
        " \n",
        "\t# now that we have the dimensions of the new image, construct\n",
        "\t# the set of destination points to obtain a \"birds eye view\",\n",
        "\t# (i.e. top-down view) of the image, again specifying points\n",
        "\t# in the top-left, top-right, bottom-right, and bottom-left\n",
        "\t# order\n",
        "      dst = np.array([\n",
        "\t\t[0, 0],\n",
        "\t\t[maxWidth - 1, 0],\n",
        "\t\t[maxWidth - 1, maxHeight - 1],\n",
        "\t\t[0, maxHeight - 1]], dtype = \"float32\")\n",
        " \n",
        "\t# compute the perspective transform matrix and then apply it\n",
        "      M = cv2.getPerspectiveTransform(rect, dst)\n",
        "      warped = cv2.warpPerspective(image, M, (maxWidth, maxHeight))\n",
        " \n",
        "\t# return the warped image\n",
        "      return warped\n",
        "  \n",
        "    def remove_close_points(self, centroids,max_point,img,removal_parameter = 0.2):\n",
        "      height, width, c = img.shape\n",
        "      possible_points = []\n",
        "      for i in centroids:\n",
        "        if (np.abs(i[0]-max_point[0]) > removal_parameter*height):\n",
        "          possible_points.append(i)\n",
        "        \n",
        "      possible_points = np.array(possible_points)  \n",
        "      return possible_points\n",
        "    \n",
        "    \n",
        "    def distance(self, pointA,pointB):\n",
        "      \n",
        "      dist = math.sqrt((pointA[0] - pointB[0])**2 + (pointA[1] - pointB[1])**2)\n",
        "      return dist\n",
        "  \n",
        "    def get_degrees(self, segment1,segment2,segment3):\n",
        "    #get angle between segment1 and 2\n",
        "      radius = (segment1 * segment1 + segment2 * segment2 - segment3 * segment3)/(2.0 * segment1 * segment2)\n",
        "      if radius > 1 :\n",
        "        angle = 180 - math.degrees(math.acos(radius - 2 ))\n",
        "      elif radius < -1 : \n",
        "        angle = 180 - math.degrees(math.acos(radius + 2 ))\n",
        "      else :\n",
        "        angle = math.degrees(math.acos(radius))\n",
        "    \n",
        "      return angle\n",
        "  \n",
        "    def point_orderer(self, rect4):\n",
        "      if rect4[0][1] < rect4[1][1] :\n",
        "        A = rect4[0]\n",
        "        B = rect4[1]\n",
        "      \n",
        "      else :\n",
        "        A = rect4[1]\n",
        "        B = rect4[0]\n",
        "        \n",
        "      if rect4[2][1] < rect4[3][1] :\n",
        "        C = rect4[3]\n",
        "        D = rect4[2]\n",
        "        \n",
        "      else :\n",
        "        C = rect4[2]\n",
        "        D = rect4[3]   \n",
        "        \n",
        "      return A,B,C,D\n",
        "  \n",
        "  \n",
        "    def correct_point(self, opposed,sided1,sided2,correct_point):\n",
        "      \n",
        "      correct_point[0] = sided1[0] + sided2[0] - opposed[0]\n",
        "      correct_point[1] = sided1[1] + sided2[1] - opposed[1]\n",
        "      \n",
        "      return correct_point\n",
        "    \n",
        "    \n",
        "    def crop_crop_strategy(self, A,B,C,D, tolerance, ratio_tolerance):\n",
        "\n",
        "    #compute sides\n",
        "      AB = self.distance(A,B)\n",
        "      BC = self.distance(B,C)\n",
        "      CD = self.distance(C,D)\n",
        "      AD = self.distance(D,A)\n",
        "    \n",
        "    #compute diags\n",
        "      AC = self.distance(A,C)\n",
        "      BD = self.distance(B,D)\n",
        "    \n",
        "      Angle_A = self.get_degrees(AB,AD,BD)\n",
        "      Angle_B = self.get_degrees(AB,BC,AC)\n",
        "      Angle_C = self.get_degrees(BC,CD,BD)\n",
        "      Angle_D = self.get_degrees(AD,CD,AC)\n",
        "    \n",
        "      ratio_A = AD/AB*abs(Angle_A-90)/100\n",
        "      ratio_B = BC/AB*abs(Angle_B-90)/100\n",
        "      ratio_C = BC/CD*abs(Angle_C-90)/100\n",
        "      ratio_D = AD/CD*abs(Angle_D-90)/100\n",
        "    \n",
        "      if abs(Angle_A - Angle_C) > tolerance:\n",
        "\n",
        "        if abs(Angle_A -90) > abs(Angle_C - 90):\n",
        "            \n",
        "            ## scenario corret A\n",
        "          if abs(ratio_A-ratio_tolerance) > abs(ratio_B-ratio_tolerance) and abs(ratio_A-ratio_tolerance) > abs(ratio_D-ratio_tolerance): \n",
        "            A = self.correct_point(C,B,D,A)\n",
        "\n",
        "        else :\n",
        "          ## scenario correect C\n",
        "          if abs(ratio_C-ratio_tolerance) > abs(ratio_B-ratio_tolerance) and abs(ratio_C-ratio_tolerance) > abs(ratio_D-ratio_tolerance)  :\n",
        "            C = self.correct_point(A,B,D,C)\n",
        "            \n",
        "      if abs(Angle_B - Angle_D) > tolerance:\n",
        "      \n",
        "        if abs(Angle_B -90) > abs(Angle_D - 90):\n",
        "            \n",
        "            ## scenario corret B\n",
        "          if abs(ratio_B-ratio_tolerance) > abs(ratio_C-ratio_tolerance) and abs(ratio_B-ratio_tolerance) > abs(ratio_A-ratio_tolerance):\n",
        "            B = self.correct_point(D,A,C,B)\n",
        "            \n",
        "        else :\n",
        "            ## scenario correect D\n",
        "          if abs(ratio_D-ratio_tolerance) > abs(ratio_C-ratio_tolerance) and abs(ratio_D-ratio_tolerance) > abs(ratio_A-ratio_tolerance) :\n",
        "            D = self.correct_point(B,A,C,D)\n",
        "\n",
        "            \n",
        "        return A,B,C,D\n",
        "  \n",
        "    def crop_correcter(self, rect4, tolerance = 0, ratio_tolerance = 0):\n",
        "    \n",
        "      A,B,C,D = self.point_orderer(rect4)\n",
        "    \n",
        "      A,B,C,D = self.crop_crop_strategy(A,B,C,D,tolerance = tolerance, ratio_tolerance = ratio_tolerance)\n",
        "\n",
        "      return A,B,C,D\n",
        "    \n",
        "    def first_cropping(self, resize_width = 70,crop_crop = 0.05):\n",
        "      \n",
        "      image = self.image\n",
        "    \n",
        "      first_height, first_width, c = image.shape\n",
        "      #Adding borders for better angle selection\n",
        "    \n",
        "      cropping_width = int(round(crop_crop*first_width))\n",
        "      cropping_height = int(round(crop_crop*first_height))\n",
        "    \n",
        "      image = image[cropping_height:first_height-cropping_height, \n",
        "                  cropping_width:first_width-cropping_width]\n",
        "      img = cv2.copyMakeBorder(np.array(image), 0, 0, 50, 50, cv2.BORDER_CONSTANT, value = [0,0,0] )\n",
        "      cropped = cv2.copyMakeBorder(np.array(image), 0, 0, 50, 50, cv2.BORDER_CONSTANT, value = [0,0,0] )\n",
        "    \n",
        "    \n",
        "      origin_height, origin_width, c = img.shape\n",
        "      transformation_ratio = origin_width/resize_width \n",
        "      img = imutils.resize(img, width = resize_width)\n",
        "#     light preprocessing: grey\n",
        "    \n",
        "      gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "    \n",
        "    ##passing in float32 for function corner harris\n",
        "      gray = np.float32(gray)\n",
        "      \n",
        "    #### OPTI PARA #######################################################################\"\"\"\n",
        "      dst = cv2.cornerHarris(gray,2,3,0.001)    \n",
        "    #result is dilated for marking the corners, not important\n",
        "      dst = cv2.dilate(dst,None)\n",
        "    \n",
        "    # Threshold for an optimal value, it may vary depending on the image.\n",
        "    # dst = metric of cornerhood\n",
        "      img[dst>0.02*dst.max()]=[0,0,255]\n",
        "    \n",
        "      pointed = img\n",
        "      dst = np.uint8(dst)\n",
        "\n",
        "#     identify where the red (0,0,255) points are\n",
        "      coord = np.where(np.all(img == (0, 0, 255), axis=-1))    \n",
        "      centroids = np.array(coord).T\n",
        "    \n",
        "      max_left = self.our_min(centroids)\n",
        "      max_right = self.our_max(centroids)\n",
        "    \n",
        "      possible_points_left =  self.remove_close_points(centroids,max_left,img)\n",
        "      possible_points_right = self.remove_close_points(centroids,max_right,img)\n",
        "        \n",
        "      possible_points_right = np.array(possible_points_right)\n",
        "    \n",
        "      second_max_left = self.our_min(possible_points_left)\n",
        "      second_max_right = self.our_max(possible_points_right)\n",
        "#     print('max left:' + str(max_left))\n",
        "\n",
        "\n",
        "        \n",
        "      rect2 = np.zeros((4, 2), dtype = \"float32\")\n",
        "      rect2[0] = max_left*transformation_ratio\n",
        "      rect2[1] = second_max_left*transformation_ratio\n",
        "      rect2[2] = max_right*transformation_ratio\n",
        "      rect2[3] = second_max_right*transformation_ratio\n",
        "\n",
        "    \n",
        "      rect4 = np.zeros((4, 2), dtype = \"float32\")\n",
        "      rect4[:,0] = rect2[:,1]\n",
        "      rect4[:,1] = rect2[:,0]\n",
        "    \n",
        "      A,B,C,D = self.crop_correcter(rect4)\n",
        "      rect2[0] = A\n",
        "      rect2[1] = B\n",
        "      rect2[2] = C\n",
        "      rect2[3] = D\n",
        "\n",
        "    \n",
        "    \n",
        "\n",
        "      warped = self.four_point_transform(cropped, rect4)\n",
        "\n",
        "      return warped\n",
        "  \n",
        "    def extractAndSaveFrame(self):\n",
        "        \"\"\"\n",
        "        Use this method to\n",
        "                1. detect and select the frame/screen.\n",
        "                2. preprocessing to only keep numbers (and remove noise).\n",
        "                3. slice the frame to only keep integer part.\n",
        "                4. save the sliced frame in dst_file_name.\n",
        "        :return: the extracted frame (np.array) if it was specified when instantiating the class.\n",
        "        \"\"\"\n",
        "        final = self.first_cropping()\n",
        "        cv2.imwrite(self.dst_file_name, final)\n",
        "        if self.return_image:\n",
        "            return final\n",
        "        else:\n",
        "            return\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beKZ6AvuWzMi",
        "colab_type": "text"
      },
      "source": [
        "We then apply the class to all our input photos and save the extracted LCD images to the Datasets_frames folder (Check your directories). We check for failed extractions for each quality level of photos (printed with the \"fail\" list)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GLL6UsZXebf",
        "colab_type": "code",
        "outputId": "02ea7174-58c1-4a81-b26d-2d35e805c19b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        }
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    if os.path.exists('Datasets_frames/'):\n",
        "        shutil.rmtree('Datasets_frames/')\n",
        "        os.makedirs('Datasets_frames/')\n",
        "    else:\n",
        "        os.makedirs('Datasets_frames/')\n",
        "\n",
        "    fail = [0, 0, 0]\n",
        "\n",
        "    for file in glob.glob('/content/drive/My Drive/Data_Analog_Digital/HQ_digital/*jpg'):\n",
        "\n",
        "        try:\n",
        "            f = frameExtractor_3(image=None,\n",
        "                     src_file_name=file,\n",
        "                     dst_file_name='Datasets_frames/' + str(file).split('/')[-1],\n",
        "                     return_image=False,\n",
        "                     output_shape=(400, 100))\n",
        "            f.extractAndSaveFrame()\n",
        "        except:\n",
        "            fail[0] += 1\n",
        "\n",
        "    for file in glob.glob('/content/drive/My Drive/Data_Analog_Digital/LQ_digital/*jpg'):\n",
        "        try:\n",
        "            f = frameExtractor_3(image=None,\n",
        "                     src_file_name=file,\n",
        "                     dst_file_name='Datasets_frames/' + str(file).split('/')[-1],\n",
        "                     return_image=False,\n",
        "                     output_shape=(400, 100))\n",
        "            f.extractAndSaveFrame()\n",
        "        except:\n",
        "            fail[1] += 1\n",
        "\n",
        "    for file in glob.glob('/content/drive/My Drive/Data_Analog_Digital/MQ_digital/*jpg'):\n",
        "        try:\n",
        "            f = frameExtractor_3(image=None,\n",
        "                     src_file_name=file,\n",
        "                     dst_file_name='Datasets_frames/' + str(file).split('/')[-1],\n",
        "                     return_image=False,\n",
        "                     output_shape=(400, 100))\n",
        "            f.extractAndSaveFrame()\n",
        "        except:\n",
        "            fail[2] += 1\n",
        "\n",
        "    print(fail)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4, 3, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-e2N6A5Xwm0",
        "colab_type": "text"
      },
      "source": [
        "Now that we have extracted the LCD from each photo, we can either follow the first or second methodology briefly described above. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_XThztdX8ph",
        "colab_type": "text"
      },
      "source": [
        "## Individual digit detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_h_gH7oafil",
        "colab_type": "text"
      },
      "source": [
        "> **1.** We need to seperate each digit within the cropped out LCD screen. To do this we simply divide the extracted LCD in four equal segments (cadran 1, 2, 3, 4). We then save each cropped digit in a folder corresponding to its digit based on the csv relating this information.\n",
        "\n",
        "> **2.** We then need to generate a dataset (array form) to pass to our neural net for training.\n",
        "\n",
        "> **3.**  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJ8oZZ2EaUO1",
        "colab_type": "text"
      },
      "source": [
        "### 1. Digit detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7yng-C1YcKN",
        "colab_type": "text"
      },
      "source": [
        "We start by constructing a class which we can pass automatically to all the LCD images we extracted.\n",
        "To extract digits we segment our cropped photos according to certain proportions. We can also go through the same process without taking into account proportions, but this only works if the digits before the decimal are extracted (with frame_extractor() instead of frame_extractor_2())."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIgeeSipPuAA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class cutDigits_3:\n",
        "\n",
        "    def __init__(self, image=None, src_file_name=None, dst_folder_name='Test_Datasets_digits', labels=None):\n",
        "        \"\"\"\n",
        "        The aim of this class is to extract digits from the frame-only preprocessed image.\n",
        "        We to delimit digits by bounding boxes.\n",
        "        We tried several approaches, but we present here the most successful one, a \"dummy\" yet efficient approach.\n",
        "        :param image: RGB image (numpy array NxMx3) of a SLICED SCREEN. If image is None, the image will be extracted from src_filename\n",
        "        :param src_file_name: filename of a SLICED SCREEN to load the source image (e.g. HQ_digital_preprocessing/0a07d2cff5beb0580bca191427e8cd6e1a0eb678.jpg)\n",
        "        :param dst_folder_name: home FOLDERname where to save the extracted digits.\n",
        "        :param last_digit: int, the number of digits you want to extract starting from the left (0 = no digits / 4 = all four digits).\n",
        "        :param labels: list, list of labels corresponding to the image, e.g. if th image shows 123.45, the labels will be ['x',1,2,3].\n",
        "        \"\"\"\n",
        "        if image is None :\n",
        "            self.image = cv2.imread(src_file_name)\n",
        "        else:\n",
        "            self.image = image\n",
        "        self.src_file_name = src_file_name\n",
        "        self.dst_folder_name = dst_folder_name\n",
        "        #self.last_digit=last_digit\n",
        "        self.labels = labels\n",
        "\n",
        "        #self.box_size = None\n",
        "    \n",
        "    def light_enhancement(self, image, light_factor = 1.7):\n",
        "      \n",
        "      im = Image.fromarray(image)\n",
        "      enhancer = ImageEnhance.Brightness(im)\n",
        "      enhanced_img = enhancer.enhance(light_factor)\n",
        "      enhanced_im = np.asarray(enhanced_img)\n",
        "      \n",
        "      return(enhanced_im)\n",
        "    \n",
        "    \n",
        "    def preprocess(self, image):\n",
        "      \n",
        "      gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "      gamma = self.light_enhancement(gray, light_factor = 3)\n",
        "      blurred = cv2.medianBlur(gamma, 3)\n",
        "      cleaned = cv2.bilateralFilter(blurred, 9, 75, 75)\n",
        "      equal = cv2.equalizeHist(cleaned)\n",
        "      thresh = cv2.threshold(equal, 45, 255, cv2.THRESH_BINARY_INV)[1]\n",
        "      eroded = cv2.erode(thresh, np.ones((4,4),np.uint8), iterations=1)\n",
        "      dilated = cv2.dilate(eroded, np.ones((6,6),np.uint8), iterations=2)\n",
        "      frame = dilated\n",
        "      \n",
        "      return frame\n",
        "    \n",
        "\n",
        "    def grayish(self):\n",
        "      \n",
        "      gray = self.image\n",
        "      gray_preproc = self.preprocess(gray)\n",
        "      \n",
        "      h = gray.shape[0]\n",
        "      w = gray.shape[1]\n",
        "      t = 0.4*w\n",
        "  \n",
        "            \n",
        "      crop_img1 = gray_preproc[int(0.05*h):h, int(0*w):int(0.15*w)]\n",
        "  \n",
        "      crop_img2 = gray_preproc[int(0.05*h):h, int(0.15*w):int(0.35*w)]\n",
        "      #crop_img2 = gray[int(0.05*h):h, int(0.15*w):int(0.3*w)]\n",
        "\n",
        "      crop_img3 = gray_preproc[int(0.05*h):h, int(0.3*w):int(0.5*w)]\n",
        "      #crop_img3 = gray[int(0.05*h):h, int(0.3*w):int(0.5*w)]\n",
        "\n",
        "      crop_img4 = gray_preproc[int(0.05*h):h, int(0.5*w):int(0.7*w)]\n",
        "      #crop_img4 = gray[int(0.05*h):h, int(0.5*w):int(0.6*w)]\n",
        "\n",
        "      \n",
        "      crops = [crop_img1, crop_img2, crop_img3, crop_img4]\n",
        "      \n",
        "      \n",
        "  \n",
        "    # These are decimals, we don't care about them.\n",
        "  \n",
        "    #crop_img5 = gray[int(0.05*h):h, int(0.6*w):int(0.8*w)]\n",
        "    #crop_img6 = gray[int(0.05*h):h, int(0.75*w):int(0.95*w)]\n",
        "  \n",
        "      return crops\n",
        "\n",
        "    def save_to_folder(self) :\n",
        "        \"\"\"\n",
        "        Use this method to save the extracted bounding boxes.\n",
        "        \"\"\"\n",
        "        if self.dst_folder_name is None :\n",
        "            return\n",
        "          \n",
        "        boxes = self.grayish()\n",
        "\n",
        "        for i in range(len(boxes)):\n",
        "          \n",
        "            if self.labels :\n",
        "                box = boxes[i]\n",
        "                label = self.labels[i]\n",
        "                src_file_name = self.src_file_name.split('/')[-1].split('.')[0]\n",
        "                dst_file_name = 'Datasets_digits/%s/%s_%s.jpg' % (label, src_file_name, str(i))\n",
        "                cv2.imwrite(dst_file_name, box)\n",
        "                \n",
        "            else:\n",
        "                pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qk42miXcZQUP",
        "colab_type": "text"
      },
      "source": [
        "We then apply this class to all the frames extracted and saved in the Datasets_frames folder and save the individual digits in the Datasets_digits folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuJfJQq1ZQqh",
        "colab_type": "code",
        "outputId": "9deae9ab-5280-43a7-8a4b-bcd624cef260",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        }
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    \n",
        "    \n",
        "    if os.path.exists('Datasets_digits/'):\n",
        "        shutil.rmtree('Datasets_digits/')\n",
        "        for i in range(0,11):\n",
        "            os.makedirs('Datasets_digits/%i' %i)\n",
        "    else:\n",
        "        for i in range(0,11):\n",
        "            os.makedirs('Datasets_digits/%i' %i)\n",
        "\n",
        "    # TODO: check why they fail\n",
        "\n",
        "    fail = 0\n",
        "\n",
        "    df = []\n",
        "    # NB: These 3 datasets were made with Excel\n",
        "    \n",
        "    suffix = \"csv\"\n",
        "    csv_directory = \"/content/drive/My Drive/Data_Analog_Digital/\"\n",
        "    csv_files = [i for i in os.listdir(csv_directory) if i.endswith( suffix )]\n",
        "    df = []\n",
        "    for i in range(len(csv_files)):\n",
        "        data = pd.read_csv(csv_directory +csv_files[i], sep=';')\n",
        "        df.append(data)\n",
        "            \n",
        "    df = pd.concat(df, axis=0)\n",
        "    df = df.replace(\"X\", 10)\n",
        "\n",
        "    for i in range(df.shape[0]):\n",
        "        line = df.iloc[i]\n",
        "        labels = [str(line.cadran_1), str(line.cadran_2), str(line.cadran_3), str(line.cadran_4)]\n",
        "        file_name = line.image\n",
        "        src_file_name = \"/content/Datasets_frames/%s\" % file_name\n",
        "        #print(line,\"\\n\",labels,\"\\n\",file_name,\"\\n\",src_file_name)\n",
        "        \n",
        "        try:\n",
        "          cutter = cutDigits_3(src_file_name = src_file_name, labels=labels)\n",
        "            #cutter.get_bounding_box_dummy()\n",
        "          cutter.save_to_folder() \n",
        "        except: \n",
        "          fail +=1\n",
        "          \n",
        "    print(fail)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "92\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2xq-SffVUtj",
        "colab_type": "text"
      },
      "source": [
        "All digits being cropped, preprocessed and associated to the correct label we can now start the dataset generation to be passed to the neural net (array form)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Vk5rBHAZdi8",
        "colab_type": "text"
      },
      "source": [
        "# Transfer learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii0RjTNOZicq",
        "colab_type": "text"
      },
      "source": [
        "We decided to consider transfer learning for digit classification. We compare results of the classifier with and without feature augmentation to benchmark our accuracy. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrhFj7TbWZLA",
        "colab_type": "text"
      },
      "source": [
        "## 1. With feature augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gn4KF9DOWyLw",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**Function**: train_transfer_learning\n",
        "\n",
        "This function calls the training of the neural network (NN).\n",
        "The neural network has been trained on the ImageNet dataset.\n",
        "We use transfer learning to use this powerful pre-trained NN, and rettrain only parts of it.\n",
        "In addition, we add a few layers to the existing network\n",
        "We trained two versions:\n",
        "1) with feature augmentation\n",
        "2) without feature augmentation\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_S7hUQuMt3t",
        "colab_type": "code",
        "outputId": "dd003adb-cf07-4e85-a253-aeec4c4da447",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        }
      },
      "source": [
        "for i in range(11):\n",
        "  folder = str(i)+\"/\"\n",
        "  new_dir = \"/content/Validation_Datasets_digits/\"+str(folder)\n",
        "  old_dir = \"/content/Datasets_digits/\"+str(folder)\n",
        "\n",
        "  for root, directories, files in os.walk(old_dir, topdown=True):\n",
        "    i = 0 \n",
        "\n",
        "    if os.path.exists(new_dir):\n",
        "      shutil.rmtree(new_dir)\n",
        "\n",
        "    if not os.path.exists(new_dir):\n",
        "      os.makedirs(new_dir)\n",
        "\n",
        "    for file in files:\n",
        "      i+=1\n",
        "      shutil.move(old_dir+str(file), \n",
        "              str(new_dir)+str(file))\n",
        "      if i == 30:\n",
        "        break\n",
        "print(\"done\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nna8gtCWlKI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_transfer_learning():\n",
        "  #import MobileNet for transfer learning:\n",
        "  base_model = MobileNet(weights='imagenet',include_top=False) #imports the mobilenet model and discards the last 1000 neuron layer.\n",
        "  x=base_model.output\n",
        "  x=GlobalAveragePooling2D()(x)\n",
        "  x=Dense(1024,activation='relu')(x) #we add dense layers so that the model can learn more complex functions and classify for better results.\n",
        "  x=Dense(1024,activation='relu')(x) #dense layer 2\n",
        "  x=Dense(512,activation='relu')(x) #dense layer 3\n",
        "  #we classify into 10 digts, so we need 10 neurons:\n",
        "  preds=Dense(11,activation='softmax')(x) #final layer with softmax activation\n",
        "  #generate model based on architecture provided:\n",
        "  model=Model(inputs=base_model.input,outputs=preds)\n",
        "  #activate transfer learning\n",
        "  for layer in model.layers[:75]:\n",
        "    layer.trainable=False\n",
        "  #defreeze layers for re-training:\n",
        "  for layer in model.layers[75:]:\n",
        "    layer.trainable=True\n",
        "  (train_generator,val_generator) = prepare_images_for_algorithm()\n",
        " #train the model:\n",
        "  model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        " # Adam optimizer\n",
        " # loss function will be categorical cross entropy\n",
        " # evaluation metric will be accuracy\n",
        "  step_size_train=train_generator.n//train_generator.batch_size\n",
        "  model_out = model.fit_generator(generator=train_generator,\n",
        "                     validation_data=val_generator,\n",
        "                    steps_per_epoch=step_size_train,\n",
        "                    epochs=40)\n",
        "  return model_out\n",
        "\n",
        "def prepare_images_for_algorithm():\n",
        " #put images into right format:\n",
        "  train_datagen=ImageDataGenerator(preprocessing_function=preprocess_input,\n",
        "                                  rotation_range=20,\n",
        "                                  width_shift_range=0.2,\n",
        "                                  height_shift_range=0.2,\n",
        "                                  horizontal_flip=True\n",
        "                                 ) #included in our dependencies\n",
        "  test_datagen=ImageDataGenerator(preprocessing_function=preprocess_input\n",
        "                                  #rotation_range=20,\n",
        "                                  #width_shift_range=0.2,\n",
        "                                  #height_shift_range=0.2,\n",
        "                                  #horizontal_flip=True\n",
        "                                ) #included in our dependencies\n",
        "  train_generator=train_datagen.flow_from_directory(\"/content/Datasets_digits\",\n",
        "                                                  target_size=(228,228),\n",
        "                                                  color_mode='rgb',\n",
        "                                                  batch_size=40,\n",
        "                                                  class_mode='categorical',\n",
        "                                                  shuffle=True)\n",
        "  val_generator=test_datagen.flow_from_directory(\"/content/Validation_Datasets_digits\",\n",
        "                                                  target_size=(228,228),\n",
        "                                                  color_mode='rgb',\n",
        "                                                  batch_size=40,\n",
        "                                                  class_mode='categorical',\n",
        "                                                  shuffle=True)\n",
        "  return(train_generator,val_generator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYsF1k8MWpS9",
        "colab_type": "code",
        "outputId": "d714a8ad-981b-4c8d-d15c-4afc10ecf1b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model_with_augmentation = train_transfer_learning()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_applications/mobilenet.py:207: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "  warnings.warn('`input_shape` is undefined or non-square, '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.6/mobilenet_1_0_224_tf_no_top.h5\n",
            "17227776/17225924 [==============================] - 2s 0us/step\n",
            "Found 2734 images belonging to 11 classes.\n",
            "Found 330 images belonging to 11 classes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Epoch 1/40\n",
            "68/68 [==============================] - 40s 589ms/step - loss: 1.5297 - acc: 0.5316 - val_loss: 2.5974 - val_acc: 0.2182\n",
            "Epoch 2/40\n",
            "68/68 [==============================] - 37s 543ms/step - loss: 1.2049 - acc: 0.6376 - val_loss: 3.6051 - val_acc: 0.2061\n",
            "Epoch 3/40\n",
            "68/68 [==============================] - 37s 541ms/step - loss: 1.0826 - acc: 0.6723 - val_loss: 1.8792 - val_acc: 0.3818\n",
            "Epoch 4/40\n",
            "68/68 [==============================] - 37s 542ms/step - loss: 1.0523 - acc: 0.6943 - val_loss: 2.1832 - val_acc: 0.3061\n",
            "Epoch 5/40\n",
            "68/68 [==============================] - 37s 541ms/step - loss: 1.0242 - acc: 0.7060 - val_loss: 2.7020 - val_acc: 0.1909\n",
            "Epoch 6/40\n",
            "68/68 [==============================] - 37s 540ms/step - loss: 0.9750 - acc: 0.7215 - val_loss: 2.2043 - val_acc: 0.2758\n",
            "Epoch 7/40\n",
            "68/68 [==============================] - 37s 543ms/step - loss: 0.9611 - acc: 0.7257 - val_loss: 2.5823 - val_acc: 0.1848\n",
            "Epoch 8/40\n",
            "68/68 [==============================] - 37s 538ms/step - loss: 0.9460 - acc: 0.7206 - val_loss: 1.9196 - val_acc: 0.3667\n",
            "Epoch 9/40\n",
            "68/68 [==============================] - 37s 548ms/step - loss: 0.9111 - acc: 0.7368 - val_loss: 2.6597 - val_acc: 0.2576\n",
            "Epoch 10/40\n",
            "68/68 [==============================] - 36s 535ms/step - loss: 0.8762 - acc: 0.7514 - val_loss: 2.2681 - val_acc: 0.2939\n",
            "Epoch 11/40\n",
            "68/68 [==============================] - 37s 548ms/step - loss: 0.8820 - acc: 0.7408 - val_loss: 2.1963 - val_acc: 0.3182\n",
            "Epoch 12/40\n",
            "68/68 [==============================] - 37s 541ms/step - loss: 0.8793 - acc: 0.7446 - val_loss: 1.8110 - val_acc: 0.4485\n",
            "Epoch 13/40\n",
            "68/68 [==============================] - 36s 537ms/step - loss: 0.8417 - acc: 0.7596 - val_loss: 2.3972 - val_acc: 0.2091\n",
            "Epoch 14/40\n",
            "68/68 [==============================] - 37s 541ms/step - loss: 0.8132 - acc: 0.7593 - val_loss: 2.3743 - val_acc: 0.2152\n",
            "Epoch 15/40\n",
            "68/68 [==============================] - 37s 542ms/step - loss: 0.8247 - acc: 0.7568 - val_loss: 1.8469 - val_acc: 0.4242\n",
            "Epoch 16/40\n",
            "68/68 [==============================] - 37s 544ms/step - loss: 0.7848 - acc: 0.7761 - val_loss: 1.9562 - val_acc: 0.3697\n",
            "Epoch 17/40\n",
            "68/68 [==============================] - 36s 530ms/step - loss: 0.8245 - acc: 0.7646 - val_loss: 1.7457 - val_acc: 0.4818\n",
            "Epoch 18/40\n",
            "68/68 [==============================] - 37s 539ms/step - loss: 0.7888 - acc: 0.7734 - val_loss: 1.9882 - val_acc: 0.3273\n",
            "Epoch 19/40\n",
            "68/68 [==============================] - 37s 541ms/step - loss: 0.7939 - acc: 0.7679 - val_loss: 2.7303 - val_acc: 0.2727\n",
            "Epoch 20/40\n",
            "68/68 [==============================] - 37s 547ms/step - loss: 0.7416 - acc: 0.7846 - val_loss: 2.5260 - val_acc: 0.1970\n",
            "Epoch 21/40\n",
            "68/68 [==============================] - 36s 535ms/step - loss: 0.7771 - acc: 0.7742 - val_loss: 2.0401 - val_acc: 0.3515\n",
            "Epoch 22/40\n",
            "68/68 [==============================] - 37s 539ms/step - loss: 0.7606 - acc: 0.7796 - val_loss: 1.8684 - val_acc: 0.3939\n",
            "Epoch 23/40\n",
            "68/68 [==============================] - 37s 541ms/step - loss: 0.7622 - acc: 0.7730 - val_loss: 1.5695 - val_acc: 0.5364\n",
            "Epoch 24/40\n",
            "68/68 [==============================] - 37s 540ms/step - loss: 0.7078 - acc: 0.7896 - val_loss: 2.0979 - val_acc: 0.2970\n",
            "Epoch 25/40\n",
            "68/68 [==============================] - 37s 542ms/step - loss: 0.7430 - acc: 0.7741 - val_loss: 2.4414 - val_acc: 0.3667\n",
            "Epoch 26/40\n",
            "68/68 [==============================] - 37s 544ms/step - loss: 0.6783 - acc: 0.8099 - val_loss: 2.1173 - val_acc: 0.3970\n",
            "Epoch 27/40\n",
            "68/68 [==============================] - 37s 537ms/step - loss: 0.7460 - acc: 0.7791 - val_loss: 1.8797 - val_acc: 0.4182\n",
            "Epoch 28/40\n",
            "68/68 [==============================] - 36s 534ms/step - loss: 0.6987 - acc: 0.7917 - val_loss: 1.7187 - val_acc: 0.4576\n",
            "Epoch 29/40\n",
            "68/68 [==============================] - 36s 534ms/step - loss: 0.6559 - acc: 0.8027 - val_loss: 2.6958 - val_acc: 0.1273\n",
            "Epoch 30/40\n",
            "68/68 [==============================] - 37s 539ms/step - loss: 0.6707 - acc: 0.8029 - val_loss: 2.2061 - val_acc: 0.3545\n",
            "Epoch 31/40\n",
            "68/68 [==============================] - 37s 539ms/step - loss: 0.6962 - acc: 0.7862 - val_loss: 4.8422 - val_acc: 0.1121\n",
            "Epoch 32/40\n",
            "68/68 [==============================] - 36s 534ms/step - loss: 0.6219 - acc: 0.8074 - val_loss: 1.9888 - val_acc: 0.4727\n",
            "Epoch 33/40\n",
            "68/68 [==============================] - 37s 539ms/step - loss: 0.6618 - acc: 0.8018 - val_loss: 1.9178 - val_acc: 0.4424\n",
            "Epoch 34/40\n",
            "68/68 [==============================] - 37s 538ms/step - loss: 0.6256 - acc: 0.8076 - val_loss: 2.1600 - val_acc: 0.3545\n",
            "Epoch 35/40\n",
            "68/68 [==============================] - 36s 534ms/step - loss: 0.6083 - acc: 0.8136 - val_loss: 1.5312 - val_acc: 0.5667\n",
            "Epoch 36/40\n",
            "68/68 [==============================] - 37s 539ms/step - loss: 0.6271 - acc: 0.8144 - val_loss: 3.0290 - val_acc: 0.2727\n",
            "Epoch 37/40\n",
            "68/68 [==============================] - 37s 544ms/step - loss: 0.6146 - acc: 0.8158 - val_loss: 2.4578 - val_acc: 0.4121\n",
            "Epoch 38/40\n",
            "68/68 [==============================] - 37s 539ms/step - loss: 0.6204 - acc: 0.8195 - val_loss: 2.1058 - val_acc: 0.4152\n",
            "Epoch 39/40\n",
            "68/68 [==============================] - 37s 539ms/step - loss: 0.5999 - acc: 0.8197 - val_loss: 1.6832 - val_acc: 0.5152\n",
            "Epoch 40/40\n",
            "68/68 [==============================] - 37s 538ms/step - loss: 0.5939 - acc: 0.8235 - val_loss: 2.4942 - val_acc: 0.3394\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TNgsELMTQzP",
        "colab_type": "code",
        "outputId": "328ba274-cfd7-48bf-bba1-907f7fcfc446",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(model_with_augmentation.history['acc'])\n",
        "plt.plot(model_with_augmentation.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nH4ys5BamMgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxI0bSLDcXuI",
        "colab_type": "text"
      },
      "source": [
        "It may be that the model is better off without data augmentation which we check for below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAWaj-44cooU",
        "colab_type": "code",
        "outputId": "f962dc5b-f4ca-4d67-a702-24d160843ea3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "model_with_augmentation"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2edb036738f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_with_augmentation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model_with_augmentation' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6l0TBU69LFn9",
        "colab_type": "text"
      },
      "source": [
        "## 2. without feature augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuS4Vo-6JXf9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\"\"\"\n",
        "-------------------------------------------------------\n",
        "Function: train_transfer_learning\n",
        "-------------------------------------------------------\n",
        "This function calls the training\n",
        "\"\"\"\n",
        "def train_transfer_learning():\n",
        "  #import MobileNet for transfer learning:\n",
        "\n",
        "  base_model = MobileNet(weights='imagenet',include_top=False) #imports the mobilenet model and discards the last 1000 neuron layer.\n",
        "\n",
        "  x=base_model.output\n",
        "  x=GlobalAveragePooling2D()(x)\n",
        "  x=Dense(1024,activation='relu')(x) #we add dense layers so that the model can learn more complex functions and classify for better results.\n",
        "  x=Dense(1024,activation='relu')(x) #dense layer 2\n",
        "  x=Dense(512,activation='relu')(x) #dense layer 3\n",
        "\n",
        "  #we classify into 10 digts, so we need 10 neurons:\n",
        "  preds=Dense(11,activation='softmax')(x) #final layer with softmax activation\n",
        "  \n",
        "  #generate model based on architecture provided:\n",
        "  model=Model(inputs=base_model.input,outputs=preds)\n",
        "  \n",
        "  #activate transfer learning\n",
        "  for layer in model.layers[:75]:\n",
        "      layer.trainable=False\n",
        "  #defreeze layers for re-training:\n",
        "  for layer in model.layers[75:]:\n",
        "      layer.trainable=True\n",
        "      \n",
        "  train_generator,val_generator = prepare_images_for_algorithm()\n",
        "  \n",
        "  #train the model:\n",
        "  model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "  # Adam optimizer\n",
        "  # loss function will be categorical cross entropy\n",
        "  # evaluation metric will be accuracy\n",
        "\n",
        "  step_size_train=train_generator.n//train_generator.batch_size\n",
        "  trained_model = model.fit_generator(generator=train_generator,\n",
        "                      validation_data=val_generator,\n",
        "                     steps_per_epoch=step_size_train,\n",
        "                     epochs=20)\n",
        "  return trained_model\n",
        "\n",
        " \n",
        "def prepare_images_for_algorithm():\n",
        "  #put images into right format:\n",
        "  train_datagen=ImageDataGenerator(preprocessing_function=preprocess_input,\n",
        "                                   #rotation_range=20,\n",
        "                                   #width_shift_range=0.2,\n",
        "                                   #height_shift_range=0.2,\n",
        "                                   #horizontal_flip=True\n",
        "                                  ) #included in our dependencies\n",
        "  test_datagen=ImageDataGenerator(preprocessing_function=preprocess_input,\n",
        "                                   #rotation_range=20,\n",
        "                                   #width_shift_range=0.2,\n",
        "                                   #height_shift_range=0.2,\n",
        "                                   #horizontal_flip=True\n",
        "                                 ) #included in our dependencies\n",
        "\n",
        "\n",
        "  train_generator=train_datagen.flow_from_directory(\"/content/Datasets_digits\",\n",
        "                                                   target_size=(228,228),\n",
        "                                                   color_mode='rgb',\n",
        "                                                   batch_size=40,\n",
        "                                                   class_mode='categorical',\n",
        "                                                   subset='training'\n",
        "                                                   #shuffle=True\n",
        "                                                   )\n",
        "\n",
        "  val_generator=train_datagen.flow_from_directory(\"/content/Datasets_digits\",\n",
        "                                                   target_size=(228,228),\n",
        "                                                   color_mode='rgb',\n",
        "                                                   batch_size=40,\n",
        "                                                   class_mode='categorical',\n",
        "                                                   subset='validation'\n",
        "                                                   #shuffle=True\n",
        "                                                )\n",
        "  \n",
        "  return(train_generator,val_generator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nOxnjdBJXi2",
        "colab_type": "code",
        "outputId": "802e9e2d-cf83-4567-a7e7-9955fa61b955",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = train_transfer_learning()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_applications/mobilenet.py:207: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "  warnings.warn('`input_shape` is undefined or non-square, '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.6/mobilenet_1_0_224_tf_no_top.h5\n",
            "17227776/17225924 [==============================] - 1s 0us/step\n",
            "Found 3064 images belonging to 11 classes.\n",
            "Found 0 images belonging to 11 classes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Epoch 1/20\n",
            "76/76 [==============================] - 16s 212ms/step - loss: 1.3744 - acc: 0.6012\n",
            "Epoch 2/20\n",
            "76/76 [==============================] - 10s 131ms/step - loss: 0.9499 - acc: 0.7275\n",
            "Epoch 3/20\n",
            "76/76 [==============================] - 10s 131ms/step - loss: 0.7975 - acc: 0.7745\n",
            "Epoch 4/20\n",
            "76/76 [==============================] - 10s 131ms/step - loss: 0.7128 - acc: 0.7869\n",
            "Epoch 5/20\n",
            "76/76 [==============================] - 10s 131ms/step - loss: 0.6074 - acc: 0.8198\n",
            "Epoch 6/20\n",
            "76/76 [==============================] - 10s 131ms/step - loss: 0.5075 - acc: 0.8518\n",
            "Epoch 7/20\n",
            "76/76 [==============================] - 10s 131ms/step - loss: 0.4479 - acc: 0.8695\n",
            "Epoch 8/20\n",
            "76/76 [==============================] - 10s 131ms/step - loss: 0.4235 - acc: 0.8803\n",
            "Epoch 9/20\n",
            "76/76 [==============================] - 10s 131ms/step - loss: 0.3869 - acc: 0.8852\n",
            "Epoch 10/20\n",
            "76/76 [==============================] - 10s 131ms/step - loss: 0.3192 - acc: 0.9075\n",
            "Epoch 11/20\n",
            "76/76 [==============================] - 10s 130ms/step - loss: 0.2993 - acc: 0.9110\n",
            "Epoch 12/20\n",
            "76/76 [==============================] - 10s 131ms/step - loss: 0.2743 - acc: 0.9204\n",
            "Epoch 13/20\n",
            "76/76 [==============================] - 10s 131ms/step - loss: 0.2472 - acc: 0.9364\n",
            "Epoch 14/20\n",
            "76/76 [==============================] - 10s 130ms/step - loss: 0.2097 - acc: 0.9457\n",
            "Epoch 15/20\n",
            "76/76 [==============================] - 10s 129ms/step - loss: 0.2526 - acc: 0.9285\n",
            "Epoch 16/20\n",
            "76/76 [==============================] - 10s 130ms/step - loss: 0.2016 - acc: 0.9476\n",
            "Epoch 17/20\n",
            "76/76 [==============================] - 10s 131ms/step - loss: 0.2012 - acc: 0.9487\n",
            "Epoch 18/20\n",
            "76/76 [==============================] - 10s 129ms/step - loss: 0.1572 - acc: 0.9554\n",
            "Epoch 19/20\n",
            "76/76 [==============================] - 10s 130ms/step - loss: 0.1779 - acc: 0.9583\n",
            "Epoch 20/20\n",
            "76/76 [==============================] - 10s 130ms/step - loss: 0.1831 - acc: 0.9522\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPewIJMuW-8R",
        "colab_type": "text"
      },
      "source": [
        "Now that we have trained our CNN we can start making predictions for the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdcA3CzabNJZ",
        "colab_type": "text"
      },
      "source": [
        "## TEST-SET GENERATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNykR5dqXGnF",
        "colab_type": "text"
      },
      "source": [
        "We first must go through the same process of extracting, cropping and pre-processing all the images before making prediction. \n",
        "\n",
        "Below is a class function re-adapted to apply the digit cropping part to a folder containing photos without labels attached. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2Ya-YIYs0Mm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class cutDigits_4:\n",
        "\n",
        "    def __init__(self, image=None, src_file_name=None, dst_folder_name='Test_Datasets_digits', labels=None):\n",
        "        \"\"\"\n",
        "        The aim of this class is to extract digits from the frame-only preprocessed image.\n",
        "        We to delimit digits by bounding boxes.\n",
        "        We tried several approaches, but we present here the most successful one, a \"dummy\" yet efficient approach.\n",
        "        :param image: RGB image (numpy array NxMx3) of a SLICED SCREEN. If image is None, the image will be extracted from src_filename\n",
        "        :param src_file_name: filename of a SLICED SCREEN to load the source image (e.g. HQ_digital_preprocessing/0a07d2cff5beb0580bca191427e8cd6e1a0eb678.jpg)\n",
        "        :param dst_folder_name: home FOLDERname where to save the extracted digits.\n",
        "        :param last_digit: int, the number of digits you want to extract starting from the left (0 = no digits / 4 = all four digits).\n",
        "        :param labels: list, list of labels corresponding to the image, e.g. if th image shows 123.45, the labels will be ['x',1,2,3].\n",
        "        \"\"\"\n",
        "        if image is None :\n",
        "            self.image = cv2.imread(src_file_name)\n",
        "        else:\n",
        "            self.image = image\n",
        "        self.src_file_name = src_file_name\n",
        "        self.dst_folder_name = dst_folder_name\n",
        "        #self.last_digit=last_digit\n",
        "        self.labels = labels\n",
        "\n",
        "        #self.box_size = None\n",
        "    \n",
        "    def light_enhancement(self, image, light_factor = 1.7):\n",
        "      \n",
        "      im = Image.fromarray(image)\n",
        "      enhancer = ImageEnhance.Brightness(im)\n",
        "      enhanced_img = enhancer.enhance(light_factor)\n",
        "      enhanced_im = np.asarray(enhanced_img)\n",
        "      \n",
        "      return(enhanced_im)\n",
        "    \n",
        "    \n",
        "    def preprocess(self, image):\n",
        "      \n",
        "      gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "      gamma = self.light_enhancement(gray, light_factor = 3)\n",
        "      blurred = cv2.medianBlur(gamma, 3)\n",
        "      cleaned = cv2.bilateralFilter(blurred, 9, 75, 75)\n",
        "      equal = cv2.equalizeHist(cleaned)\n",
        "      thresh = cv2.threshold(equal, 45, 255, cv2.THRESH_BINARY_INV)[1]\n",
        "      eroded = cv2.erode(thresh, np.ones((4,4),np.uint8), iterations=1)\n",
        "      dilated = cv2.dilate(eroded, np.ones((6,6),np.uint8), iterations=2)\n",
        "      frame = dilated\n",
        "      \n",
        "      return frame\n",
        "    \n",
        "\n",
        "    def grayish(self):\n",
        "      \n",
        "      gray = self.image\n",
        "      gray_preproc = self.preprocess(gray)\n",
        "      \n",
        "      h = gray.shape[0]\n",
        "      w = gray.shape[1]\n",
        "      t = 0.4*w\n",
        "  \n",
        "            \n",
        "      crop_img1 = gray_preproc[int(0.05*h):h, int(0*w):int(0.15*w)]\n",
        "  \n",
        "      crop_img2 = gray_preproc[int(0.05*h):h, int(0.15*w):int(0.35*w)]\n",
        "      #crop_img2 = gray[int(0.05*h):h, int(0.15*w):int(0.3*w)]\n",
        "\n",
        "      crop_img3 = gray_preproc[int(0.05*h):h, int(0.3*w):int(0.5*w)]\n",
        "      #crop_img3 = gray[int(0.05*h):h, int(0.3*w):int(0.5*w)]\n",
        "\n",
        "      crop_img4 = gray_preproc[int(0.05*h):h, int(0.5*w):int(0.7*w)]\n",
        "      #crop_img4 = gray[int(0.05*h):h, int(0.5*w):int(0.6*w)]\n",
        "\n",
        "      \n",
        "      crops = [crop_img1, crop_img2, crop_img3, crop_img4]\n",
        "      \n",
        "      \n",
        "  \n",
        "    # These are decimals, we don't care about them.\n",
        "  \n",
        "    #crop_img5 = gray[int(0.05*h):h, int(0.6*w):int(0.8*w)]\n",
        "    #crop_img6 = gray[int(0.05*h):h, int(0.75*w):int(0.95*w)]\n",
        "  \n",
        "      return crops\n",
        "\n",
        "    def save_to_folder(self) :\n",
        "        \"\"\"\n",
        "        Use this method to save the extracted bounding boxes.\n",
        "        \"\"\"\n",
        "        if self.dst_folder_name is None :\n",
        "            return\n",
        "          \n",
        "        boxes = self.grayish()\n",
        "\n",
        "        for i in range(len(boxes)):\n",
        "          \n",
        "            if self.labels :\n",
        "                box = boxes[i]\n",
        "                label = self.labels[i]\n",
        "                src_file_name = self.src_file_name.split('/')[-1].split('.')[0]\n",
        "                dst_file_name = 'Test_Datasets_digits/%s/%s_%s.jpg' % (label, src_file_name, str(i))\n",
        "                cv2.imwrite(dst_file_name, box)\n",
        "                \n",
        "            else:\n",
        "                pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iz_E3PuWXgo5",
        "colab_type": "text"
      },
      "source": [
        "We start by applying the frame extraction method:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWfaFoD9bMR4",
        "colab_type": "code",
        "outputId": "99b786f6-58a7-4f1b-f525-6cec72be7a3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    if os.path.exists('Test_Datasets_frames/'):\n",
        "        shutil.rmtree('Test_Datasets_frames/')\n",
        "        os.makedirs('Test_Datasets_frames/')\n",
        "    else:\n",
        "        os.makedirs('Test_Datasets_frames/')\n",
        "\n",
        "    fail = [0, 0, 0]\n",
        "\n",
        "    for file in glob.glob('/content/drive/My Drive/Test_Data_Analog_Digital/HQ_digital_test/*jpg'):\n",
        "\n",
        "        try:\n",
        "            f = frameExtractor_3(image=None,\n",
        "                     src_file_name=file,\n",
        "                     dst_file_name='Test_Datasets_frames/' + str(file).split('/')[-1],\n",
        "                     return_image=False,\n",
        "                     output_shape=(400, 100))\n",
        "            f.extractAndSaveFrame()\n",
        "        except:\n",
        "            fail[0] += 1\n",
        "\n",
        "    for file in glob.glob('/content/drive/My Drive/Test_Data_Analog_Digital/LQ_digital_test/*jpg'):\n",
        "        try:\n",
        "            f = frameExtractor_3(image=None,\n",
        "                     src_file_name=file,\n",
        "                     dst_file_name='Test_Datasets_frames/' + str(file).split('/')[-1],\n",
        "                     return_image=False,\n",
        "                     output_shape=(400, 100))\n",
        "            f.extractAndSaveFrame()\n",
        "        except:\n",
        "            fail[1] += 1\n",
        "\n",
        "    for file in glob.glob('/content/drive/My Drive/Test_Data_Analog_Digital/MQ_digital_test/*jpg'):\n",
        "        try:\n",
        "            f = frameExtractor_3(image=None,\n",
        "                     src_file_name=file,\n",
        "                     dst_file_name='Test_Datasets_frames/' + str(file).split('/')[-1],\n",
        "                     return_image=False,\n",
        "                     output_shape=(400, 100))\n",
        "            f.extractAndSaveFrame()\n",
        "        except:\n",
        "            fail[2] += 1\n",
        "\n",
        "    print(fail)\n",
        "print(\"done\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 2, 0]\n",
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHHu0etVYFeh",
        "colab_type": "text"
      },
      "source": [
        "We then proceed to crop the digits with the adapted class:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXXS0KRAb9pD",
        "colab_type": "code",
        "outputId": "3a395447-b040-4141-c783-e16d219f95f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    \n",
        "    \n",
        "    if os.path.exists('Test_Datasets_digits/'):\n",
        "        shutil.rmtree('Test_Datasets_digits/')\n",
        "        for i in range(1,5):\n",
        "            os.makedirs('Test_Datasets_digits/%i' %i)\n",
        "    else:\n",
        "        for i in range(1,5):\n",
        "            os.makedirs('Test_Datasets_digits/%i' %i)\n",
        "\n",
        "    # TODO: check why they fail\n",
        "\n",
        "    fail = 0\n",
        "    \n",
        "    labels = [1,2,3,4]\n",
        "    fail = 0\n",
        "  \n",
        "    for file in glob.glob('/content/Test_Datasets_frames/*jpg'):        \n",
        "      try:\n",
        "        cutter = cutDigits_4(src_file_name = file, labels=labels)\n",
        "              #cutter.get_bounding_box_dummy()\n",
        "        cutter.save_to_folder() \n",
        "      except:\n",
        "\n",
        "        fail +=1\n",
        "\n",
        "    print(fail)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smH6fnDwYQaM",
        "colab_type": "text"
      },
      "source": [
        "We can now move on to predicting every digit individually to reconstruct the final number from the photo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioEkLLyOhu_T",
        "colab_type": "text"
      },
      "source": [
        "## Make prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMtZuZcVzptk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from itertools import compress\n",
        "\n",
        "folder_1 = sorted(glob.glob(\"/content/Test_Datasets_digits/1/*jpg\"))\n",
        "folder_2 = sorted(glob.glob(\"/content/Test_Datasets_digits/2/*jpg\"))\n",
        "folder_3 = sorted(glob.glob(\"/content/Test_Datasets_digits/3/*jpg\"))\n",
        "folder_4 = sorted(glob.glob(\"/content/Test_Datasets_digits/4/*jpg\"))\n",
        "\n",
        "folder_global = []\n",
        "folder_global = [folder_1,folder_2,folder_3,folder_4]\n",
        "\n",
        "final_pred_list = []\n",
        "pic_name_list = []\n",
        "number_pred_str = \"\"\n",
        "for num_picture in range(len(folder_1)):\n",
        "  count = 0\n",
        "  for folder in folder_global:\n",
        "    pic_name = str(folder[num_picture]).replace('_0','').replace('_1','').replace('_2','').replace('_3','').split('/')[-1]\n",
        "    line = df.loc[df['image'] == pic_name].used_liter.item()\n",
        "    count = count + 1\n",
        "    \n",
        "    img = cv2.imread(folder[num_picture])\n",
        "    img2 = cv2.resize(img,(228,228))\n",
        "    img3 = np.reshape(img2,(1,228,228,3))\n",
        "\n",
        "    pred = (model.model.predict(img3))\n",
        "\n",
        "    max_prob = max(pred[0][:])\n",
        "    ind = (pred[0][:] == max_prob)\n",
        "\n",
        "    \n",
        "    prediction_list = [\"0\",\"1\",\"X\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
        "    prediction_str = str(list(compress(prediction_list,ind))[0])\n",
        "    if count == 1:\n",
        "      prediction_str = \"X\"\n",
        "    number_pred_str = number_pred_str + prediction_str\n",
        "    number_pred_list.append(prediction_str)\n",
        "    pic_name = str(folder[num_picture]).replace('_0','').replace('_1','').replace('_2','').replace('_3','').split('/')[-1]\n",
        "    \n",
        "    \n",
        "    \n",
        "    if count == 4:\n",
        "      final_pred_list.append(number_pred_str)\n",
        "      number_pred_str = \"\"\n",
        "      pic_name_list.append(pic_name)\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ASNWe3KQNnp",
        "colab_type": "code",
        "outputId": "afe94303-4522-47da-bd5a-c0cc04ca207d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "RESULT = pd.DataFrame()\n",
        "RESULT[\"prediction\"] = final_pred_list\n",
        "RESULT[\"file\"] = folder_1\n",
        "\n",
        "RESULT.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prediction</th>\n",
              "      <th>file</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>X5X5</td>\n",
              "      <td>/content/Test_Datasets_digits/1/00b527335f956f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>X515</td>\n",
              "      <td>/content/Test_Datasets_digits/1/00e22973fdbc19...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>X266</td>\n",
              "      <td>/content/Test_Datasets_digits/1/00f35ec57ceabe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>XX16</td>\n",
              "      <td>/content/Test_Datasets_digits/1/00f531ce72874d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>XX55</td>\n",
              "      <td>/content/Test_Datasets_digits/1/02b292a500dfb9...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>XX55</td>\n",
              "      <td>/content/Test_Datasets_digits/1/04dc6be4599a36...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>X151</td>\n",
              "      <td>/content/Test_Datasets_digits/1/0a008e3b1506c5...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XX56</td>\n",
              "      <td>/content/Test_Datasets_digits/1/0a0216c77785d9...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>X256</td>\n",
              "      <td>/content/Test_Datasets_digits/1/0a07d2cff5beb0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>X115</td>\n",
              "      <td>/content/Test_Datasets_digits/1/0a1e1c676aa31a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>XX11</td>\n",
              "      <td>/content/Test_Datasets_digits/1/0a32664b2b8c8f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>X515</td>\n",
              "      <td>/content/Test_Datasets_digits/1/0a5f85a46d4095...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>XX56</td>\n",
              "      <td>/content/Test_Datasets_digits/1/0a6e4d5f38965d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>X515</td>\n",
              "      <td>/content/Test_Datasets_digits/1/0a72d48cfd9758...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>X122</td>\n",
              "      <td>/content/Test_Datasets_digits/1/0a737aefecf923...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>XXX5</td>\n",
              "      <td>/content/Test_Datasets_digits/1/0a7387ad1c3c8c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>XX25</td>\n",
              "      <td>/content/Test_Datasets_digits/1/0a8b40c58493a3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>X212</td>\n",
              "      <td>/content/Test_Datasets_digits/1/0a9323377845ae...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>XX55</td>\n",
              "      <td>/content/Test_Datasets_digits/1/0aa6fa634d2cce...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>XXXX</td>\n",
              "      <td>/content/Test_Datasets_digits/1/0ab5f86e4d729d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>X116</td>\n",
              "      <td>/content/Test_Datasets_digits/1/0abe9f0f27e8b4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>XX55</td>\n",
              "      <td>/content/Test_Datasets_digits/1/0ad51a9afc621e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>X266</td>\n",
              "      <td>/content/Test_Datasets_digits/1/0af4f227dc701f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>XX55</td>\n",
              "      <td>/content/Test_Datasets_digits/1/0afd972947328a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>X551</td>\n",
              "      <td>/content/Test_Datasets_digits/1/0b10bbdc8da66f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>X516</td>\n",
              "      <td>/content/Test_Datasets_digits/1/0b1b3d01f3cc8c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>XX16</td>\n",
              "      <td>/content/Test_Datasets_digits/1/0b2ec6a6fa1ec2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>X211</td>\n",
              "      <td>/content/Test_Datasets_digits/1/0b69e687253c1d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>X266</td>\n",
              "      <td>/content/Test_Datasets_digits/1/0b7d12c697c812...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>XX55</td>\n",
              "      <td>/content/Test_Datasets_digits/1/0b7d7c92f3a0f9...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>X255</td>\n",
              "      <td>/content/Test_Datasets_digits/1/0d740a7b8f5a55...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>X215</td>\n",
              "      <td>/content/Test_Datasets_digits/1/0d920b3d5f881f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>X296</td>\n",
              "      <td>/content/Test_Datasets_digits/1/0d9a0966ffe42c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>XX55</td>\n",
              "      <td>/content/Test_Datasets_digits/1/0dd592299667e7...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>X515</td>\n",
              "      <td>/content/Test_Datasets_digits/1/0e11679f05b72f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>X265</td>\n",
              "      <td>/content/Test_Datasets_digits/1/0e3273c2747f5c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>XX66</td>\n",
              "      <td>/content/Test_Datasets_digits/1/0e403444174002...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>X256</td>\n",
              "      <td>/content/Test_Datasets_digits/1/0e40a9dec5f22d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>X266</td>\n",
              "      <td>/content/Test_Datasets_digits/1/0eb5e984725415...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>X225</td>\n",
              "      <td>/content/Test_Datasets_digits/1/0ec67bbc3e1d1c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>X262</td>\n",
              "      <td>/content/Test_Datasets_digits/1/0f7d9a795212d9...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>XX65</td>\n",
              "      <td>/content/Test_Datasets_digits/1/0fc1317374529c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>XX56</td>\n",
              "      <td>/content/Test_Datasets_digits/1/1bc7bbefa14f6d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>X219</td>\n",
              "      <td>/content/Test_Datasets_digits/1/1d0cef2855b988...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>XX52</td>\n",
              "      <td>/content/Test_Datasets_digits/1/1eec400baaac81...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>X225</td>\n",
              "      <td>/content/Test_Datasets_digits/1/1f1ec8eee75d49...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>X111</td>\n",
              "      <td>/content/Test_Datasets_digits/1/1f27a79030d2b4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>X111</td>\n",
              "      <td>/content/Test_Datasets_digits/1/2b1f5ea851600e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>XX15</td>\n",
              "      <td>/content/Test_Datasets_digits/1/2b6481132ed176...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>XX55</td>\n",
              "      <td>/content/Test_Datasets_digits/1/2bf3406560e9dc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>X221</td>\n",
              "      <td>/content/Test_Datasets_digits/1/2c07716d1d18e0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>X116</td>\n",
              "      <td>/content/Test_Datasets_digits/1/2f4e7043428941...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>XX15</td>\n",
              "      <td>/content/Test_Datasets_digits/1/3c823ec6e7a740...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>X255</td>\n",
              "      <td>/content/Test_Datasets_digits/1/3dda11f98b920b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>X155</td>\n",
              "      <td>/content/Test_Datasets_digits/1/4a322d88fca63d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>X555</td>\n",
              "      <td>/content/Test_Datasets_digits/1/4ac652ff81459d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>X665</td>\n",
              "      <td>/content/Test_Datasets_digits/1/4b9a7c65f5c42e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>XXXX</td>\n",
              "      <td>/content/Test_Datasets_digits/1/4c14ffddd8e9ca...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>XX66</td>\n",
              "      <td>/content/Test_Datasets_digits/1/4cf115ed8fca14...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>X195</td>\n",
              "      <td>/content/Test_Datasets_digits/1/4de40c9f2697c6...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>83 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   prediction                                               file\n",
              "0        X5X5  /content/Test_Datasets_digits/1/00b527335f956f...\n",
              "1        X515  /content/Test_Datasets_digits/1/00e22973fdbc19...\n",
              "2        X266  /content/Test_Datasets_digits/1/00f35ec57ceabe...\n",
              "3        XX16  /content/Test_Datasets_digits/1/00f531ce72874d...\n",
              "4        XX55  /content/Test_Datasets_digits/1/02b292a500dfb9...\n",
              "5        XX55  /content/Test_Datasets_digits/1/04dc6be4599a36...\n",
              "6        X151  /content/Test_Datasets_digits/1/0a008e3b1506c5...\n",
              "7        XX56  /content/Test_Datasets_digits/1/0a0216c77785d9...\n",
              "8        X256  /content/Test_Datasets_digits/1/0a07d2cff5beb0...\n",
              "9        X115  /content/Test_Datasets_digits/1/0a1e1c676aa31a...\n",
              "10       XX11  /content/Test_Datasets_digits/1/0a32664b2b8c8f...\n",
              "11       X515  /content/Test_Datasets_digits/1/0a5f85a46d4095...\n",
              "12       XX56  /content/Test_Datasets_digits/1/0a6e4d5f38965d...\n",
              "13       X515  /content/Test_Datasets_digits/1/0a72d48cfd9758...\n",
              "14       X122  /content/Test_Datasets_digits/1/0a737aefecf923...\n",
              "15       XXX5  /content/Test_Datasets_digits/1/0a7387ad1c3c8c...\n",
              "16       XX25  /content/Test_Datasets_digits/1/0a8b40c58493a3...\n",
              "17       X212  /content/Test_Datasets_digits/1/0a9323377845ae...\n",
              "18       XX55  /content/Test_Datasets_digits/1/0aa6fa634d2cce...\n",
              "19       XXXX  /content/Test_Datasets_digits/1/0ab5f86e4d729d...\n",
              "20       X116  /content/Test_Datasets_digits/1/0abe9f0f27e8b4...\n",
              "21       XX55  /content/Test_Datasets_digits/1/0ad51a9afc621e...\n",
              "22       X266  /content/Test_Datasets_digits/1/0af4f227dc701f...\n",
              "23       XX55  /content/Test_Datasets_digits/1/0afd972947328a...\n",
              "24       X551  /content/Test_Datasets_digits/1/0b10bbdc8da66f...\n",
              "25       X516  /content/Test_Datasets_digits/1/0b1b3d01f3cc8c...\n",
              "26       XX16  /content/Test_Datasets_digits/1/0b2ec6a6fa1ec2...\n",
              "27       X211  /content/Test_Datasets_digits/1/0b69e687253c1d...\n",
              "28       X266  /content/Test_Datasets_digits/1/0b7d12c697c812...\n",
              "29       XX55  /content/Test_Datasets_digits/1/0b7d7c92f3a0f9...\n",
              "..        ...                                                ...\n",
              "53       X255  /content/Test_Datasets_digits/1/0d740a7b8f5a55...\n",
              "54       X215  /content/Test_Datasets_digits/1/0d920b3d5f881f...\n",
              "55       X296  /content/Test_Datasets_digits/1/0d9a0966ffe42c...\n",
              "56       XX55  /content/Test_Datasets_digits/1/0dd592299667e7...\n",
              "57       X515  /content/Test_Datasets_digits/1/0e11679f05b72f...\n",
              "58       X265  /content/Test_Datasets_digits/1/0e3273c2747f5c...\n",
              "59       XX66  /content/Test_Datasets_digits/1/0e403444174002...\n",
              "60       X256  /content/Test_Datasets_digits/1/0e40a9dec5f22d...\n",
              "61       X266  /content/Test_Datasets_digits/1/0eb5e984725415...\n",
              "62       X225  /content/Test_Datasets_digits/1/0ec67bbc3e1d1c...\n",
              "63       X262  /content/Test_Datasets_digits/1/0f7d9a795212d9...\n",
              "64       XX65  /content/Test_Datasets_digits/1/0fc1317374529c...\n",
              "65       XX56  /content/Test_Datasets_digits/1/1bc7bbefa14f6d...\n",
              "66       X219  /content/Test_Datasets_digits/1/1d0cef2855b988...\n",
              "67       XX52  /content/Test_Datasets_digits/1/1eec400baaac81...\n",
              "68       X225  /content/Test_Datasets_digits/1/1f1ec8eee75d49...\n",
              "69       X111  /content/Test_Datasets_digits/1/1f27a79030d2b4...\n",
              "70       X111  /content/Test_Datasets_digits/1/2b1f5ea851600e...\n",
              "71       XX15  /content/Test_Datasets_digits/1/2b6481132ed176...\n",
              "72       XX55  /content/Test_Datasets_digits/1/2bf3406560e9dc...\n",
              "73       X221  /content/Test_Datasets_digits/1/2c07716d1d18e0...\n",
              "74       X116  /content/Test_Datasets_digits/1/2f4e7043428941...\n",
              "75       XX15  /content/Test_Datasets_digits/1/3c823ec6e7a740...\n",
              "76       X255  /content/Test_Datasets_digits/1/3dda11f98b920b...\n",
              "77       X155  /content/Test_Datasets_digits/1/4a322d88fca63d...\n",
              "78       X555  /content/Test_Datasets_digits/1/4ac652ff81459d...\n",
              "79       X665  /content/Test_Datasets_digits/1/4b9a7c65f5c42e...\n",
              "80       XXXX  /content/Test_Datasets_digits/1/4c14ffddd8e9ca...\n",
              "81       XX66  /content/Test_Datasets_digits/1/4cf115ed8fca14...\n",
              "82       X195  /content/Test_Datasets_digits/1/4de40c9f2697c6...\n",
              "\n",
              "[83 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 285
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzDNjJwQYfHR",
        "colab_type": "text"
      },
      "source": [
        "Above should appear a dataframe containing the prediction for every photo within your dedicated test folder, with the according directory to each of them."
      ]
    }
  ]
}