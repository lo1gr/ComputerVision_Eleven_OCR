{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Seven-segment digit classification.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"9bYS_KhqOkyW","colab_type":"text"},"source":["# Seven segment digit classification"]},{"cell_type":"markdown","metadata":{"id":"zX1bgJ9wP85n","colab_type":"text"},"source":["This notebook contains the various methods we considered to identify numbers composed of seven segment digits in photos. \n","\n","The following are the methodologies we considiered:\n","\n","\n","  1. Extract LCD screen, extract each digit withing the LCD and classify them seperately\n","\n","\n","  2. Extract LCD screen and classify the number as a whole.\n","  \n","  \n","There are several options to extract the LCD screen after pre-processing each image:\n","\n","\n","> - Detect angles within each photo, the angles the most to the left and right of the photo should approximately correspond to each corner of the LCD screen.\n","\n","> - Using the cv2 package, find the contours within each photo that correspond to the biggest four segment object. If this method fails we simply crop a rectangle from the middle of the photo.\n","\n","\n","For the first method, we considered a simple approach to crop out each digit individually out of the extracted LCD screen. We simply divide the LCD in four equal segments. The crops are then passed to a deep neural net which will classify each digit individually. \n","\n","\n","For the second method, the LCD screen is passed to a deep neural net which will both detect and classify the numbers accordignly. \n"]},{"cell_type":"markdown","metadata":{"id":"HStpT9ttOsGU","colab_type":"text"},"source":["## Mount drive to load and save data"]},{"cell_type":"code","metadata":{"id":"wq0nelhkO4Ja","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":74},"outputId":"1d6b6a24-98f4-466e-9223-e3a6f3305a3f","executionInfo":{"status":"ok","timestamp":1569165376380,"user_tz":-120,"elapsed":397,"user":{"displayName":"Alexis LAKS","photoUrl":"","userId":"13982579254934744342"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"b7aVy9OiOvbU","colab_type":"text"},"source":["## Load packages"]},{"cell_type":"code","metadata":{"id":"85XdzOlWOkH9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"dd76d065-88e6-41bf-fe76-8d08db2d7417","executionInfo":{"status":"ok","timestamp":1569161976148,"user_tz":-120,"elapsed":3214,"user":{"displayName":"Alexis LAKS","photoUrl":"","userId":"13982579254934744342"}}},"source":["import logging\n","import shutil\n","import imutils\n","import glob\n","import os\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib\n","import matplotlib.pyplot as plt \n","matplotlib.use('agg')\n","\n","import cv2\n","import skimage.filters as ft\n","import scipy.spatial as sp\n","from skimage.measure import label, regionprops\n","from PIL import Image\n","\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","import keras.backend\n","from keras.models import Sequential\n","from keras.models import Model\n","from keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten\n","from keras.layers.core import Dropout, Activation\n","from keras.layers import BatchNormalization\n","from keras import regularizers\n","from keras.optimizers import Adam\n","from keras.utils import plot_model\n","from keras.callbacks import TensorBoard,EarlyStopping\n","from keras.backend.tensorflow_backend import set_session"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"hpdLJnziWEH1","colab_type":"text"},"source":["## Extracting LCD screen"]},{"cell_type":"markdown","metadata":{"id":"9hLmnRMtXi1o","colab_type":"text"},"source":["### Extraction with contour identification"]},{"cell_type":"markdown","metadata":{"id":"W0FFFqM_WlQx","colab_type":"text"},"source":["First we define a class which we can apply automatically to all our input images. Note that all the preprocessing is integrated directly in this class and applied to each photo to increase chances of correctly isolating the LCD."]},{"cell_type":"code","metadata":{"id":"FAXXl0t2Wk8M","colab_type":"code","colab":{}},"source":["class frameExtractor:\n","\n","    def __init__(self, image=None, src_file_name=None, dst_file_name=None, return_image=False, output_shape =(400,100)):\n","        \"\"\"\n","        Use this class to extract the frame/LCD screen from the image. This is our step 1 for image preprocessing.\n","        The final frame is extracted in grayscale.\n","        Note that it works for the \"digital\" case and can be used for the \"analog\" case, but it is more efficient on the \"digital\" case.\n","        :param image: RGB image (numpy array NxMx3) with a screen to extract. If image is None, the image will be extracted from src_filename\n","        :param src_file_name: filename to load the source image where the screen needs to be extracted (e.g. HQ_digital/0a07d2cff5beb0580bca191427e8cd6e1a0eb678.jpg)\n","        :param dst_file_name: filename to save the preprocessed image (e.g. HQ_digital_frame/0a07d2cff5beb0580bca191427e8cd6e1a0eb678.jpg\n","        :param return_image: a boolean, if True extractAndSave returns an image (np. array) / if False it just saves the image.\n","        :param output_shape: shape (in pxl) of the output image.\n","        \"\"\"\n","        if image is None :\n","            self.image = cv2.imread(src_file_name)\n","        else :\n","            self.image = image\n","        self.dst_file_name = dst_file_name\n","        self.return_image = return_image\n","        self.output_shape = output_shape\n","        self.raw_frame = None\n","        self.frame = None\n","        self.sliced_frame = None\n","\n","\n","    def distance_from_center(self, rectangle):\n","        \"\"\"\n","        Use this function to measure how far a rectangle is from the center of an image.\n","        Most of the time the frame is approx. in the middle of the picture.\n","        Note that the code works for shapes that are approx. rectangles.\n","        :param rectangle: a 4x2 array with the coordinates of each corner of the rectangle.\n","        :return: the distance (a float) between the center of the rectangle and the center of the picture.\n","        \"\"\"\n","        center_rc = 0.5*(rectangle[0]+ rectangle[2])\n","        center_image = 0.5*np.array([self.image.shape[1],self.image.shape[0]])\n","        distance = np.linalg.norm(center_rc-center_image)\n","        return distance\n","\n","\n","\n","    def sort_pts_clockwise(A):\n","        \"\"\"\n","        Use this function to sort in clockwise order points in R^2.\n","        Credit: https://stackoverflow.com/questions/30088697/4-1-2-numpy-array-sort-clockwise\n","        :param A: a Nx2 array with the 2D coordinates of the points to sort.\n","        :return: a Nx2 array with the points sorted in clockwise order starting with the top-left point.\n","        \"\"\"\n","        # Sort A based on Y(col-2) coordinates\n","        sortedAc2 = A[np.argsort(A[:,1]),:]\n","        # Get top two and bottom two points\n","        top2 = sortedAc2[0:2,:]\n","        bottom2 = sortedAc2[2:,:]\n","        # Sort top2 points to have the first row as the top-left one\n","        sortedtop2c1 = top2[np.argsort(top2[:,0]),:]\n","        top_left = sortedtop2c1[0,:]\n","        # Use top left point as pivot & calculate sq-euclidean dist against\n","        # bottom2 points & thus get bottom-right, bottom-left sequentially\n","        sqdists = sp.distance.cdist(top_left[None], bottom2, 'sqeuclidean')\n","        rest2 = bottom2[np.argsort(np.max(sqdists,0))[::-1],:]\n","        # Concatenate all these points for the final output\n","        return np.concatenate((sortedtop2c1,rest2),axis =0)\n","\n","\n","    def adjust_gamma(image, gamma=1.0):\n","        \"\"\"\n","        Use this function to adjust illumination in an image.\n","        Credit: https://stackoverflow.com/questions/33322488/how-to-change-image-illumination-in-opencv-python\n","        :param image: A grayscale image (NxM int array in [0, 255]\n","        :param gamma: A positive float. If gamma<1 the image is darken / if gamma>1 the image is enlighten / if gamma=1 nothing happens.\n","        :return: the enlighten/darken version of image\n","        \"\"\"\n","        invGamma = 1.0 / gamma\n","        table = np.array([((i / 255.0) ** invGamma) * 255 for i in np.arange(0, 256)])\n","        return cv2.LUT(image.astype(np.uint8), table.astype(np.uint8))\n","\n","\n","    def frameDetection(self):\n","        \"\"\"\n","        The core method of the class. Use it to extract the frame in the image.\n","        The extracted frame is in grayscale.\n","        The followed steps are :\n","            1. grayscale + smoothering + gamma to make the frame darker + binary threshold (rational = the frame is one of the darkest part in the picture).\n","            2. extract regions of \"interest\".\n","            3. heuristic to find a region of interest that is large enough, in the center of the picture and where length along x-axis > length along y-axis.\n","            4. make a perspective transform to crop the image and deal with perspective deformations.\n","        \"\"\"\n","        self.image = imutils.resize(self.image, height=500)\n","\n","        # Step 1: grayscale + smoothering + gamma to make the frame darker + binary threshold\n","        gray = cv2.cvtColor(self.image, cv2.COLOR_BGR2GRAY)\n","        blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n","        gamma = frameExtractor.adjust_gamma(blurred, gamma=0.7)\n","        shapeMask = cv2.threshold(gamma, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n","\n","        # Step 2: extract regions of \"interest\".\n","        label_image = label(shapeMask)\n","\n","        Cnt = None\n","        position = [0, 0, 0, 0]\n","\n","        for region in regionprops(label_image):\n","            # Step 3: heuristic to find a region large enough, in the center & with length along x-axis > length along y-axis.\n","            minr, minc, maxr, maxc = region.bbox\n","            c = np.array([[minc, minr], [minc, maxr], [maxc, minr], [maxc, maxr]])\n","\n","            if Cnt is None:\n","                Cnt = c\n","                position = [minr, minc, maxr, maxc]\n","\n","            old_dist = self.distance_from_center(Cnt)\n","            new_dist = self.distance_from_center(c)\n","\n","            Lx = maxc - minc\n","            Ly = maxr - minr\n","\n","            c = frameExtractor.sort_pts_clockwise(c)\n","\n","            if old_dist>new_dist and Ly<Lx and cv2.contourArea(c)>0.05*(shapeMask.shape[0]*shapeMask.shape[1]):\n","                displayCnt = c\n","                position = [minr, minc, maxr, maxc]\n","\n","        Cnt = Cnt.reshape(4, 2)\n","        Cnt = frameExtractor.sort_pts_clockwise(Cnt)\n","\n","\n","        # Step 4: Make a perspective transform to crop the image and deal with perspective deformations.\n","        try:\n","            # Crop the image around the region of interest (but keep a bit of distance with a 30px padding).\n","            # Darken + Binary threshold + rectangle detection.\n","            # If this technique fails, raise an error and use basic methods (except part).\n","\n","            crop_img = self.image[max(0, position[0] - 30):min(position[2] + 30, self.image.shape[0]),\\\n","                       max(0, position[1] - 30):min(self.image.shape[1], position[3] + 30)]\n","\n","            crop_blurred = cv2.GaussianBlur(crop_img, (5, 5), 0)\n","            crop_gamma = frameExtractor.adjust_gamma(crop_blurred, gamma=0.4)\n","            crop_gray = cv2.cvtColor(crop_gamma, cv2.COLOR_BGR2GRAY)\n","            crop_thresh = cv2.threshold(crop_gray, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n","\n","            cnts = cv2.findContours(crop_thresh.copy(), cv2.RETR_EXTERNAL,\n","                                    cv2.CHAIN_APPROX_SIMPLE)\n","            cnts = cnts[0] if imutils.is_cv2() else cnts[1]\n","            cnts = sorted(cnts, key=cv2.contourArea, reverse=True)\n","            Cnt_bis = None\n","\n","            for c in cnts:\n","                peri = cv2.arcLength(c, True)\n","                approx = cv2.approxPolyDP(c, 0.02 * peri, True)\n","\n","                if len(approx) == 4:\n","                    Cnt_bis = approx\n","                    break\n","\n","            if cv2.contourArea(Cnt_bis)<0.5*(crop_img.shape[0]*crop_img.shape[1]):\n","                raise ValueError(\"Couldn't find the box, so switching to ad hoc method.\")\n","\n","            Cnt_bis = Cnt_bis.reshape(4, 2)\n","            Cnt_bis = frameExtractor.sort_pts_clockwise(Cnt_bis)\n","            src_pts = Cnt_bis.copy()\n","            src_pts = src_pts.astype(np.float32)\n","\n","            dst_pts = np.array([[0, 0], [400, 0], [400, 100], [0, 100]], dtype=np.float32)\n","            dst_pts = dst_pts.astype(np.float32)\n","\n","            persp = cv2.getPerspectiveTransform(src_pts, dst_pts)\n","            warped = cv2.warpPerspective(crop_img, persp, (400, 100))\n","\n","\n","        except:\n","            # More basic techniques that give +/- acceptable results when the first technique fails.\n","            src_pts = Cnt.copy()\n","            src_pts = src_pts.astype(np.float32)\n","\n","            dst_pts = np.array([[0, 0], [400, 0], [400, 100], [0, 100]], dtype=np.float32)\n","            dst_pts = dst_pts.astype(np.float32)\n","\n","            persp = cv2.getPerspectiveTransform(src_pts, dst_pts)\n","            warped = cv2.warpPerspective(gray, persp, (400, 100))\n","\n","        # Frame is extracted from the initial image in grayscale (not other processing done on the image).\n","        self.raw_frame = warped\n","\n","\n","    # TODO : check why they fail\n","    \"\"\"\n","    http://www.amphident.de/en/blog/preprocessing-for-automatic-pattern-identification-in-wildlife-removing-glare.html\n","    http://people.csail.mit.edu/yichangshih/mywebsite/reflection.pdf\n","    http://news.mit.edu/2015/algorithm-removes-reflections-photos-0511\n","    \"\"\"\n","    def preprocessFrame(self):\n","        \"\"\"\n","        Final preprocessing that outputs a clean image 'cleaned_img' with more contrasts\n","        \"\"\"\n","        try :\n","            gray = cv2.cvtColor(self.raw_frame, cv2.COLOR_BGR2GRAY)\n","        except :\n","            gray = self.raw_frame\n","        thresh = cv2.equalizeHist(gray)\n","        thresh = cv2.threshold(thresh, 45, 255, cv2.THRESH_BINARY_INV)[1]\n","        cleaned_img = cv2.dilate(thresh, None, iterations=1)\n","        self.frame = cleaned_img\n","\n","\n","    def sliceFrame(self):\n","        \"\"\"\n","        Use this method to slice the frame and only keep the integer part (e.g. 123.45 becomes 123).\n","        Heuristic: comma is approx. at 8/13 of the image.\n","        :return:\n","        \"\"\"\n","        stop_at = int(np.floor(self.output_shape[0]*8/13))\n","        self.sliced_frame = np.array(self.frame)[:,:stop_at]\n","\n","\n","    def extractAndSaveFrame(self):\n","        \"\"\"\n","        Use this method to\n","                1. detect and select the frame/screen.\n","                2. preprocessing to only keep numbers (and remove noise).\n","                3. slice the frame to only keep integer part.\n","                4. save the sliced frame in dst_file_name.\n","        :return: the extracted frame (np.array) if it was specified when instantiating the class.\n","        \"\"\"\n","        self.frameDetection()\n","        self.preprocessFrame()\n","        self.sliceFrame()\n","        cv2.imwrite(self.dst_file_name, self.sliced_frame)\n","        if self.return_image:\n","            return self.sliced_frame\n","        else:\n","            return"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"beKZ6AvuWzMi","colab_type":"text"},"source":["We then apply the class to all our input photos and save the extracted LCD images to the Datasets_frames folder (Check your directories). We check for failed extractions for each quality level of photos (printed with the \"fail\" list)"]},{"cell_type":"code","metadata":{"id":"hyqZ7oG5Vxlk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"a391a041-2e6a-4786-de5e-f76eb2876d96","executionInfo":{"status":"ok","timestamp":1569165344449,"user_tz":-120,"elapsed":403,"user":{"displayName":"Alexis LAKS","photoUrl":"","userId":"13982579254934744342"}}},"source":["if __name__ == \"__main__\":\n","\n","    if os.path.exists('Datasets_frames/'):\n","        shutil.rmtree('Datasets_frames/')\n","        os.makedirs('Datasets_frames/')\n","    else:\n","        os.makedirs('Datasets_frames/')\n","\n","    fail = [0, 0, 0]\n","\n","    for file in glob.glob('/content/drive/My Drive/Data_Analog_Digital/HQ_digital/*jpg'):\n","\n","        try:\n","            f = frameExtractor(image=None,\n","                               src_file_name=file,\n","                               dst_file_name='Datasets_frames/' + str(file).split('/')[-1],\n","                               return_image=False,\n","                               output_shape=(400, 100))\n","            f.extractAndSaveFrame()\n","        except:\n","            fail[0] += 1\n","\n","    for file in glob.glob('/content/drive/My Drive/Data_Analog_Digital/LQ_digital/*jpg'):\n","        try:\n","            f = frameExtractor(image=None,\n","                               src_file_name=file,\n","                               dst_file_name='Datasets_frames/' + str(file).split('/')[-1],\n","                               return_image=False,\n","                               output_shape=(400, 100))\n","            f.extractAndSaveFrame()\n","        except:\n","            fail[1] += 1\n","\n","    for file in glob.glob('/content/drive/My Drive/Data_Analog_Digital/MQ_digital/*jpg'):\n","        try:\n","            f = frameExtractor(image=None,\n","                               src_file_name=file,\n","                               dst_file_name='Datasets_frames/' + str(file).split('/')[-1],\n","                               return_image=False,\n","                               output_shape=(400, 100))\n","            f.extractAndSaveFrame()\n","        except:\n","            fail[2] += 1\n","\n","    print(fail)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["[0, 0, 0]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pISCLNxNXpGu","colab_type":"text"},"source":["### Extraction with angle identification"]},{"cell_type":"code","metadata":{"id":"1GLL6UsZXebf","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o-e2N6A5Xwm0","colab_type":"text"},"source":["Now that we have extracted the LCD from each photo, we can either follow the first or second methodology briefly described above. "]},{"cell_type":"markdown","metadata":{"id":"o_XThztdX8ph","colab_type":"text"},"source":["## Method 1: Individual digit detection"]},{"cell_type":"markdown","metadata":{"id":"k_h_gH7oafil","colab_type":"text"},"source":["> **1.** We need to seperate each digit within the cropped out LCD screen. To do this we simply divide the extracted LCD in four equal segments (cadran 1, 2, 3, 4). We then save each cropped digit in a folder corresponding to its digit based on the csv relating this information.\n","\n","> **2.** We then need to generate a dataset (array form) to pass to our neural net for training.\n","\n","> **3.** Finally we define the architecture of our neural net and pass the previously generated dataset to train. \n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"fJ8oZZ2EaUO1","colab_type":"text"},"source":["### 1. Digit detection"]},{"cell_type":"markdown","metadata":{"id":"l7yng-C1YcKN","colab_type":"text"},"source":["We start by constructing a class which we can pass automatically to all the LCD images we extracted."]},{"cell_type":"code","metadata":{"id":"tDvo2lapX8CM","colab_type":"code","colab":{}},"source":["class cutDigits:\n","\n","    def __init__(self, image=None, src_file_name=None, dst_folder_name='Datasets_digits', last_digit=4, labels=None):\n","        \"\"\"\n","        The aim of this class is to extract digits from the frame-only preprocessed image.\n","        We to delimit digits by bounding boxes.\n","        We tried several approaches, but we present here the most successful one, a \"dummy\" yet efficient approach.\n","        :param image: RGB image (numpy array NxMx3) of a SLICED SCREEN. If image is None, the image will be extracted from src_filename\n","        :param src_file_name: filename of a SLICED SCREEN to load the source image (e.g. HQ_digital_preprocessing/0a07d2cff5beb0580bca191427e8cd6e1a0eb678.jpg)\n","        :param dst_folder_name: home FOLDERname where to save the extracted digits.\n","        :param last_digit: int, the number of digits you want to extract starting from the left (0 = no digits / 4 = all four digits).\n","        :param labels: list, list of labels corresponding to the image, e.g. if th image shows 123.45, the labels will be ['x',1,2,3].\n","        \"\"\"\n","        if image is None :\n","            self.image = cv2.imread(src_file_name)\n","        else:\n","            self.image = image\n","        self.src_file_name = src_file_name\n","        self.dst_folder_name = dst_folder_name\n","        self.last_digit=last_digit\n","        self.labels = labels\n","\n","        self.box_size = None\n","        self.boxes = []\n","\n","\n","\n","    def get_bounding_box_dummy(self):\n","        \"\"\"\n","        Use this method to get bounding boxes and extract numbers by dividing the area in 4 equal parts (\"dummy\" yet efficient approach).\n","        \"\"\"\n","\n","        self.boxes = []\n","        self.box_size = self.image.shape[1]/4\n","\n","        for i in range(self.last_digit):\n","            inf = i * self.box_size\n","            sup = (i+1) * self.box_size\n","            self.boxes += [self.image[:, int(inf):int(sup)]]\n","\n","\n","    def save_to_folder(self) :\n","        \"\"\"\n","        Use this method to save the extracted bounding boxes.\n","        \"\"\"\n","        if self.dst_folder_name is None :\n","            return\n","\n","        for i in range(len(self.boxes)):\n","            if self.labels :\n","                box = self.boxes[i]\n","                label = self.labels[i]\n","                src_file_name = self.src_file_name.split('/')[-1].split('.')[0]\n","                dst_file_name = 'Datasets_digits/%s/%s_%s.jpg' % (label, src_file_name, str(i))\n","                cv2.imwrite(dst_file_name, box)\n","                \n","            else:\n","                pass\n","\n","            #else :\n","          #      box = self.boxes[i]\n","           #     src_file_name = self.src_file_name.split('/')[-1].split('.')[0]\n","            #    dst_file_name = 'Datasets_digits/%s/%s_%s.jpg' % ('missing_label', src_file_name, str(i))\n","            #    cv2.imwrite(dst_file_name, box)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qk42miXcZQUP","colab_type":"text"},"source":["We then apply this class to all the frames extracted and saved in the Datasets_frames folder and save the individual digits in the Datasets_digits folder."]},{"cell_type":"code","metadata":{"id":"MuJfJQq1ZQqh","colab_type":"code","colab":{}},"source":["if __name__ == \"__main__\":\n","    \n","    \n","    if os.path.exists('Datasets_digits/'):\n","        shutil.rmtree('Datasets_digits/')\n","        for i in range(0,11):\n","            os.makedirs('Datasets_digits/%i' %i)\n","    else:\n","        for i in range(0,11):\n","            os.makedirs('Datasets_digits/%i' %i)\n","\n","    # TODO: check why they fail\n","\n","    fail = 0\n","\n","    df = []\n","    # NB: These 3 datasets were made with Excel\n","    \n","    suffix = \"csv\"\n","    csv_directory = \"/content/drive/My Drive/Data_Analog_Digital/\"\n","    csv_files = [i for i in os.listdir(csv_directory) if i.endswith( suffix )]\n","    df = []\n","    for i in range(len(csv_files)):\n","        data = pd.read_csv(csv_directory +csv_files[i], sep=';', index_col = 0)\n","        df.append(data)\n","            \n","    df = pd.concat(df, axis=0)\n","    df = df.replace(\"X\", 10)\n","\n","    for i in range(df.shape[0]):\n","        line = df.iloc[i]\n","        labels = [line.cadran_1, line.cadran_2, line.cadran_3, line.cadran_4]\n","        file_name = line.image\n","        src_file_name = \"content/Datasets_frames/%s\" % file_name\n","\n","        try :\n","            cutter = cutDigits(src_file_name=\"/\" + src_file_name, labels=labels)\n","            cutter.get_bounding_box_dummy()\n","            cutter.save_to_folder()\n","\n","        except :\n","            fail += 1\n","\n","    print(fail)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lO5cpZOKbAZf","colab_type":"text"},"source":["### 2. Dataset generation"]},{"cell_type":"markdown","metadata":{"id":"mX0dCj0gZoyU","colab_type":"text"},"source":["We now need to generate a dataset to train our neural net on. We again consider a class to automate this process to all images we have."]},{"cell_type":"code","metadata":{"id":"YNVvNmtQZoS9","colab_type":"code","colab":{}},"source":["class Dataset:\n","    \n","    def __init__(self):\n","            self.data = self.full_data()\n","                        \n","    def full_data(self):\n","        suffix = \".csv\"\n","        csv_directory = '/content/drive/My Drive/Data_Analog_Digital/'\n","        csv_files = [i for i in os.listdir(csv_directory) if i.endswith( suffix )]\n","        full_data = []\n","        for i in range(len(csv_files)):\n","            data = pd.read_csv(csv_directory +csv_files[i], sep=';', index_col = 0)\n","            full_data.append(data)\n","            \n","        full_data = pd.concat(full_data, axis=0)\n","        full_data = full_data.replace(\"X\", 10)\n","        return full_data\n","        \n","class Dataset_Multi(Dataset):\n","    \n","    def __init__(self):\n","        Dataset.__init__(self)\n","        self.frame_directory = '/content/Datasets_frames/'\n","        self.frame_data = self.data[self.data[\"image\"].isin(os.listdir(self.frame_directory))]\n","        \n","    def convert_to_arrays(self,samples):\n","        X = []\n","        for sample in samples:\n","            ID =  'Datasets_frames/' + \"%s\" % (sample)\n","            img = Image.open(ID)\n","            img = np.array(img)\n","            img = img.reshape((img.shape[0],img.shape[1],1))\n","            X.append(img)\n","        X = np.asarray(X)\n","        return X"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xicqPvUBbWbO","colab_type":"text"},"source":["This dataset can then be passed to the neural net for training."]},{"cell_type":"markdown","metadata":{"id":"t45aZC_eba8W","colab_type":"text"},"source":["### 3. Defining neural net architecture"]},{"cell_type":"markdown","metadata":{"id":"KnbdZitlcRly","colab_type":"text"},"source":["The architecture of this neural net is a result of general reccomendations online and in several published articles related to some extent to our topic."]},{"cell_type":"code","metadata":{"id":"M8kAAkQfbabo","colab_type":"code","colab":{}},"source":["class Model(object):\n","    \n","    def __init__(self):\n","        \n","        self.data_init()\n","        self.model_init()\n","    \n","    def data_init(self):\n","        pass\n","    \n","    def model_init(self):\n","        pass\n","    \n","    def train_predict(self):\n","        pass\n","\n","class Model_Multi(Model):\n","    \n","    def __init__(self):\n","        Model.__init__(self)\n"," \n","    def data_init(self):\n","        self.dataset = Dataset_Multi()\n","        self.data = self.dataset.frame_data\n","        self.X =  self.data.iloc[:,0]\n","        self.y = self.data.iloc[:,1:]\n","        \n","        self.ids_train, self.ids_val, self.y_train, self.y_val = train_test_split(self.X, self.y, test_size=0.25, random_state=1)        \n","        self.y_train_vect = [self.y_train[\"cadran_1\"], self.y_train[\"cadran_2\"], self.y_train[\"cadran_3\"], self.y_train[\"cadran_4\"]]\n","        self.y_val_vect =  [self.y_val[\"cadran_1\"], self.y_val[\"cadran_2\"], self.y_val[\"cadran_3\"], self.y_val[\"cadran_4\"]]\n","        \n","        self.X_train = self.dataset.convert_to_arrays(self.ids_train)\n","        self.X_val = self.dataset.convert_to_arrays(self.ids_val)\n","              \n","    def model_init(self):\n","\n","        model_input = Input((100,246,1))\n","\n","        x = Conv2D(32, (3, 3), padding='same', name='conv2d_hidden_1', kernel_regularizer=regularizers.l2(0.01))(model_input)\n","        x = BatchNormalization()(x)\n","        x = Activation('relu')(x)\n","        x = MaxPooling2D(pool_size=(2, 2), strides=(3, 3),name='maxpool_2d_hidden_1')(x)\n","        x = Dropout(0.30)(x)\n","\n","        x = Conv2D(64, (3, 3), padding='same', name='conv2d_hidden_2', kernel_regularizer=regularizers.l2(0.01))(x)\n","        x = BatchNormalization()(x)\n","        x = Activation('relu')(x)\n","        x = MaxPooling2D(pool_size=(2, 2), strides=(3, 3),name='maxpool_2d_hidden_2')(x)\n","        x = Dropout(0.30)(x)\n","\n","        x = Conv2D(128, (3, 3), padding='same', name='conv2d_hidden_3', kernel_regularizer=regularizers.l2(0.01))(x)\n","        x = BatchNormalization()(x)\n","        x = Activation('relu')(x)\n","        x = MaxPooling2D(pool_size=(2, 2), strides=(3, 3),name='maxpool_2d_hidden_3')(x)\n","        x = Dropout(0.30)(x)\n","\n","        x = Flatten()(x)\n","\n","        x = Dense(256, activation ='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n","\n","        digit1 = (Dense(output_dim =11,activation = 'softmax', name='digit_1'))(x)\n","        digit2 = (Dense(output_dim =11,activation = 'softmax', name='digit_2'))(x)\n","        digit3 = (Dense(output_dim =11,activation = 'softmax', name='digit_3'))(x)\n","        digit4 = (Dense(output_dim =11,activation = 'softmax', name='digit_4'))(x)\n","\n","        outputs = [digit1, digit2, digit3, digit4]\n","\n","        self.model = keras.models.Model(input = model_input , output = outputs)\n","        self.model._make_predict_function()\n","        \n","    def train(self, lr = 1e-3, epochs=50):\n","        optimizer = Adam(lr=lr, decay=lr/10)\n","        self.model.compile(loss=\"sparse_categorical_crossentropy\", optimizer= optimizer, metrics = ['accuracy'])\n","        keras.backend.get_session().run(tf.initialize_all_variables())\n","        self.history = self.model.fit(self.X_train, self.y_train_vect, batch_size= 50, nb_epoch=epochs, verbose=1, validation_data=(self.X_val, self.y_val_vect))\n","        \n","        \n","    def plot_loss(self):\n","        \n","        for i in range(1,5):\n","            plt.figure(figsize=[8,6])\n","            plt.plot(self.history.history['digit_%i_loss' %i],'r',linewidth=0.5)\n","            plt.plot(self.history.history['val_digit_%i_loss' %i],'b',linewidth=0.5)\n","            plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n","            plt.xlabel('Epochs ',fontsize=16)\n","            plt.ylabel('Loss',fontsize=16)\n","            plt.title('Loss Curves Digit %i' %i,fontsize=16)\n","            plt.show()\n","        \n","        \n","      \n","\n","    def plot_acc(self):\n","        \n","        for i in range(1,5):\n","            plt.figure(figsize=[8,6])\n","            plt.plot(self.history.history['digit_%i_acc' %i],'r',linewidth=0.5)\n","            plt.plot(self.history.history['val_digit_%i_acc' %i],'b',linewidth=0.5)\n","            plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=18)\n","            plt.xlabel('Epochs ',fontsize=16)\n","            plt.ylabel('Accuracy',fontsize=16)\n","            plt.title('Accuracy Curves Digit %i' %i,fontsize=16)\n","            plt.show()\n","        \n","\n","    def predict(self):\n","        self.y_pred = self.model.predict(self.X_val)\n","        correct_preds = 0\n","        \n","        for i in range(self.X_val.shape[0]):\n","            pred_list_i = [np.argmax(pred[i]) for pred in self.y_pred]\n","            val_list_i  = self.y_val.values[i].astype('int')\n","            if np.array_equal(val_list_i, pred_list_i):\n","                correct_preds = correct_preds + 1\n","            print('exact accuracy', correct_preds / self.X_val.shape[0])\n","            \n","        mse = 0 \n","        diff = []\n","        for i in range(self.X_val.shape[0]):\n","                pred_list_i = [np.argmax(pred[i]) for pred in self.y_pred]\n","                pred_number = 1000* pred_list_i[0] + 100* pred_list_i[1] + 10 * pred_list_i[2] + 1* pred_list_i[3]\n","                val_list_i  = self.y_val.values[i].astype('int')\n","                val_number = 1000* val_list_i[0] + 100*  val_list_i[1] + 10 *  val_list_i[2] + 1*  val_list_i[3]\n","                diff.append(val_number - pred_number)\n","        print('difference label vs. prediction', diff)\n","\n","    \n","    def train_predict(self):\n","        \n","        self.train()\n","        self.plot_loss()\n","        self.plot_acc()\n","        self.predict()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d-FItAEoca4S","colab_type":"text"},"source":["This class can then simply be launched with a one liner to train on all the photos we have. Before we enter this process, we will showcase the second more simple methodology we considered which does not crop each digit. "]},{"cell_type":"markdown","metadata":{"id":"7-QzdG5jc7Gk","colab_type":"text"},"source":["## Method 2: LCD number recognition"]},{"cell_type":"markdown","metadata":{"id":"fGCvbFSOeBKX","colab_type":"text"},"source":["In this methodology, we train the neural net to not only classify digits but also find a way to detect them on its own within the extracted LCD. "]},{"cell_type":"markdown","metadata":{"id":"viYttQcZeRTY","colab_type":"text"},"source":["### 1. Data generation"]},{"cell_type":"code","metadata":{"id":"Ho3QKeXTciMO","colab_type":"code","colab":{}},"source":["class Dataset:\n","    \n","    def __init__(self):\n","            self.data = self.full_data()\n","                        \n","    def full_data(self):\n","        suffix = \".csv\"\n","        csv_directory = '/content/drive/My Drive/Data_Analog_Digital/'\n","        csv_files = [i for i in os.listdir(csv_directory) if i.endswith( suffix )]\n","        full_data = []\n","        for i in range(len(csv_files)):\n","            data = pd.read_csv(csv_directory +csv_files[i], sep=';', index_col = 0)\n","            full_data.append(data)\n","            \n","        full_data = pd.concat(full_data, axis=0)\n","        full_data = full_data.replace(\"X\", 10)\n","        return full_data        \n","        \n","class Dataset_Single(Dataset):\n","    \n","    def __init__(self):\n","        Dataset.__init__(self)\n","        self.digits_directory = '/content/Datasets_digits/'\n","        self.digits_data = self.digits_data()\n","\n","    \n","    def digits_data(self):\n","        ids = []\n","        labels = []\n","        for i in range(11):\n","            directory = self.digits_directory + '%i/' %i\n","            for j in os.listdir(directory):\n","                ids.append(directory+j)\n","                labels.append(i)\n","        digits_data = pd.DataFrame(list(zip(ids,labels)))\n","        \n","        return digits_data \n","                \n","    def convert_to_arrays(self,samples):\n","        X = []\n","        for sample in samples:\n","            img = Image.open(sample)\n","            img = np.array(img)\n","            img.resize((100,256))\n","            img = img.reshape((img.shape[0],img.shape[1],1))\n","            X.append(img)\n","        X = np.asarray(X)\n","        return X"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q2wqmASTgX7y","colab_type":"text"},"source":["### 2. Model architecture definition"]},{"cell_type":"markdown","metadata":{"id":"J3cieBQYgtga","colab_type":"text"},"source":["Again the architecture of this neural net is a result of general reccomendations online and in published articles related to some extent to our topic."]},{"cell_type":"code","metadata":{"id":"BJjBYgbggSo1","colab_type":"code","colab":{}},"source":["class Model(object):\n","    \n","    def __init__(self):\n","        \n","        self.data_init()\n","        self.model_init()\n","    \n","    def data_init(self):\n","        pass\n","    \n","    def model_init(self):\n","        pass\n","    \n","    def train_predict(self):\n","        pass\n","        \n","class Model_Single(Model):\n","    \n","    \n","    def __init__(self):\n","            Model.__init__(self)\n","\n","    def data_init(self):\n","        self.dataset = Dataset_Single()\n","\n","        self.data = self.dataset.digits_data         \n","        self.X =  self.data.iloc[:,0]\n","        self.y = self.data.iloc[:,1]\n","\n","        self.ids_train, self.ids_val, self.y_train, self.y_val = train_test_split(self.X, self.y, test_size=0.25, random_state=1)\n","        self.X_train = self.dataset.convert_to_arrays(self.ids_train)\n","        self.X_val = self.dataset.convert_to_arrays(self.ids_val)\n","\n","    def model_init(self):\n","\n","\n","        model_input = Input((100, 256, 1))\n","        x = Conv2D(32, (3, 3), padding='same', name='conv2d_hidden_1', kernel_regularizer=regularizers.l2(0.01))(model_input)\n","        x = BatchNormalization()(x)\n","        x = Activation('relu')(x)\n","        x = MaxPooling2D(pool_size=(2, 2), strides=(3, 3),name='maxpool_2d_hidden_1')(x)\n","        x = Dropout(0.30)(x)\n","\n","        x = Conv2D(63, (3, 3), padding='same', name='conv2d_hidden_2', kernel_regularizer=regularizers.l2(0.01))(x)\n","        x = BatchNormalization()(x)\n","        x = Activation('relu')(x)\n","        x = MaxPooling2D(pool_size=(2, 2), strides=(3, 3),name='maxpool_2d_hidden_2')(x)\n","        x = Dropout(0.30)(x)\n","\n","        x = Conv2D(128, (3, 3), padding='same', name='conv2d_hidden_3', kernel_regularizer=regularizers.l2(0.01))(x)\n","        x = BatchNormalization()(x)\n","        x = Activation('relu')(x)\n","        x = MaxPooling2D(pool_size=(2, 2), strides=(3, 3),name='maxpool_2d_hidden_3')(x)\n","        x = Dropout(0.30)(x)\n","\n","        x = Flatten()(x)\n","\n","        x = Dense(1024, activation ='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n","\n","        output = Dense(output_dim =11,activation = 'softmax', name='output')(x)\n","\n","        self.model = keras.models.Model(input = model_input , output = output)\n","        self.model._make_predict_function() \n","\n","    def train(self, lr = 1e-3, epochs=5):\n","        optimizer = Adam(lr=lr, decay=lr/10)\n","        self.model.compile(loss=\"sparse_categorical_crossentropy\", optimizer= optimizer, metrics = ['accuracy'])\n","        keras.backend.get_session().run(tf.initialize_all_variables())\n","        self.history = self.model.fit(self.X_train, self.y_train, batch_size= 32, nb_epoch=30, verbose=1, validation_data=(self.X_val, self.y_val))\n","\n","\n","    def plot_acc(self):\n","        plt.figure(figsize=[8,6])\n","        plt.plot(self.history.history['acc'],'r',linewidth=0.5)\n","        plt.plot(self.history.history['val_acc'],'b',linewidth=0.5)\n","        plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=18)\n","        plt.xlabel('Epochs ',fontsize=16)\n","        plt.ylabel('Accuracy',fontsize=16)\n","        plt.title('Accuracy Curves Digit',fontsize=16)\n","        plt.show()\n","\n","    def plot_loss(self):        \n","        plt.figure(figsize=[8,6])\n","        plt.plot(self.history.history['loss'],'r',linewidth=0.5)\n","        plt.plot(self.history.history['val_loss'],'b',linewidth=0.5)\n","        plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n","        plt.xlabel('Epochs ',fontsize=16)\n","        plt.ylabel('Loss',fontsize=16)\n","        plt.title('Loss Curves Digit',fontsize=16)\n","        plt.show()\n","\n","    def predict(self):\n","        \n","        self.y_pred = self.model.predict(self.X_val)\n","        \n","        ids = []\n","        pred_list = []\n","        val_list = []\n","\n","        for i in range(self.X_val.shape[0]):\n","            self.val_id = self.ids_val.values[i]\n","            ids.append(str(self.val_id.split('/')[2].split('-')[0][:-1]))\n","            pred_list_i = np.argmax(self.y_pred[i]).astype('int')\n","            pred_list.append(pred_list_i)\n","            val_list_i  = self.y_val.values[i].astype('int')\n","            val_list.append(val_list_i) \n","\n","        q = []\n","\n","        for i in np.unique(ids):\n","            q.append([i, np.where(np.isin(ids,i))[0]])\n","\n","        correct_count = 0 \n","        for i in range(len(q)):\n","            v = []\n","            p = []\n","            for j in range(len((q[i][1]))):\n","                idx = (q[i][1][j])\n","                val_list_i = val_list[idx]\n","                pred_list_i = pred_list[idx]\n","                v.append(val_list_i)\n","                p.append(pred_list_i)\n","            if np.array_equal(p, v):\n","                correct_count = correct_count + 1\n","        print('real_acc', correct_count /self.X_val.shape[0])\n","\n","\n","    def train_predict(self):\n","\n","        self.train()\n","        self.plot_loss()\n","        self.plot_acc()\n","        self.predict()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WOHCgBHmg0iI","colab_type":"text"},"source":["This class is adapted as the previous one to train on a set of photos automatically with a one-liner."]},{"cell_type":"markdown","metadata":{"id":"SZfdLplWg_vh","colab_type":"text"},"source":["## Training & Evaluation"]},{"cell_type":"markdown","metadata":{"id":"HyegWrtkhG4d","colab_type":"text"},"source":["In this section we launch and evaluate both our models for comparison. "]},{"cell_type":"markdown","metadata":{"id":"i4syl37GhLhZ","colab_type":"text"},"source":["### Model 1: Individual digit detection"]},{"cell_type":"code","metadata":{"id":"6Eg5k4H-g7Ek","colab_type":"code","colab":{}},"source":["model_1 = Model_Multi()\n","model_1.train_predict()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XcSVLKHzhTNz","colab_type":"text"},"source":["### Model 2: LCD number recognition"]},{"cell_type":"code","metadata":{"id":"PK-IENg5hWtM","colab_type":"code","colab":{}},"source":["model_2 = Model_Single()\n","model_2.train_predict()"],"execution_count":0,"outputs":[]}]}